{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'road', 'selective_logging', 'bare_ground', 'slash_burn', 'blooming', 'partly_cloudy', 'habitation', 'agriculture', 'conventional_mine', 'blow_down', 'haze', 'clear', 'water', 'cloudy', 'artisinal_mine', 'cultivation', 'primary'}\n",
      "{'agriculture': 0, 'artisinal_mine': 1, 'bare_ground': 2, 'blooming': 3, 'blow_down': 4, 'clear': 5, 'cloudy': 6, 'conventional_mine': 7, 'cultivation': 8, 'habitation': 9, 'haze': 10, 'partly_cloudy': 11, 'primary': 12, 'road': 13, 'selective_logging': 14, 'slash_burn': 15, 'water': 16}\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/planet\\planet/train_classes.csv\") # Adjust as needed\n",
    "df\n",
    "all_tags = set()\n",
    "for tags in df['tags'].str.split():\n",
    "    all_tags.update(tags)\n",
    "print(all_tags)\n",
    "tag_to_idx = {tag: idx for idx, tag in enumerate(sorted(all_tags))}\n",
    "idx_to_tag = {idx: tag for tag, idx in tag_to_idx.items()}\n",
    "print(tag_to_idx)\n",
    "print(len(tag_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet & DenseNet\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# EfficientNet_b1\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize(256),\n",
    "#     transforms.CenterCrop(240),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelImageDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None):\n",
    "        # if type(csv_file) != str:\n",
    "        #     self.df = csv_file\n",
    "        # else:\n",
    "        #     self.df = pd.read_csv(csv_file)\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx, 0]\n",
    "        if \"aug\" in img_name:\n",
    "            img_path = os.path.join(\"data/augmented_images/\", f\"{img_name}.jpg\")\n",
    "            #img_path = os.path.join(self.img_dir, f\"{img_name}\")\n",
    "        else:\n",
    "            img_path = os.path.join(self.img_dir, f\"{img_name}.jpg\")\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        tags = self.df.iloc[idx, 1].split()\n",
    "        \n",
    "        labels = torch.zeros(len(tag_to_idx))\n",
    "        for tag in tags:\n",
    "            labels[tag_to_idx[tag]] = 1\n",
    "        \n",
    "        return image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torch import nn\n",
    "\n",
    "num_classes = 17\n",
    "\n",
    "def ResNetClassifier(num_classes):\n",
    "    # load a pre-trained model\n",
    "    model_ft = models.resnet50(weights='DEFAULT')\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    \n",
    "    # freeze all the parameters in the network except the final layer\n",
    "    # for param in model_ft.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    \n",
    "    # replace the last fully connected layer\n",
    "    model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    return model_ft\n",
    "\n",
    "def EfficientNetClassifier(num_classes):\n",
    "    # load a pre-trained model\n",
    "    model_ft = models.efficientnet_b1(weights='DEFAULT')\n",
    "    # num_ftrs = model_ft.classifier.in_features\n",
    "    \n",
    "    # freeze all the parameters in the network except the final layer\n",
    "    # for param in model_ft.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    \n",
    "    # replace the last fully connected layer\n",
    "    model_ft.classifier = nn.Linear(1280, num_classes)\n",
    "    return model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation group before anything happens\n",
    "train, val1 = train_test_split(df, test_size=0.1)\n",
    "train, val2 = train_test_split(df, test_size=0.1)\n",
    "train, val3 = train_test_split(df, test_size=0.1)\n",
    "train, val4 = train_test_split(df, test_size=0.1)\n",
    "train, val5 = train_test_split(df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32787\n",
      "3644\n"
     ]
    }
   ],
   "source": [
    "df_to_load = train\n",
    "dataset = MultiLabelImageDataset(df_to_load, img_dir=\"data/planet\\planet/train-jpg\", transform=transform)\n",
    "train_idx, test_idx = train_test_split(list(range(len(dataset))), test_size=0.1, random_state=42)\n",
    "# Since valiation split already done above with original data so no need\n",
    "\n",
    "# # Only quick testing\n",
    "#train_idx_small = train_idx[:1000]  # First 1000 training samples\n",
    "#test_idx_small = test_idx[:200]  # First 200 testing samples\n",
    "\n",
    "#train_idx_small = train_idx[:32787] \n",
    "#test_idx_small = test_idx[:3644] \n",
    "\n",
    "#train_dataset = Subset(dataset, train_idx_small)\n",
    "#test_dataset = Subset(dataset, test_idx_small)\n",
    "train_dataset = Subset(dataset, list(range(len(df_to_load))))\n",
    "val1_dataset = MultiLabelImageDataset(val1, img_dir=\"data/planet\\planet/train-jpg\", transform=transform)\n",
    "val1_dataset = Subset(val1_dataset, list(range(len(val1))))\n",
    "val2_dataset = MultiLabelImageDataset(val2, img_dir=\"data/planet\\planet/train-jpg\", transform=transform)\n",
    "val2_dataset = Subset(val2_dataset, list(range(len(val2))))\n",
    "val3_dataset = MultiLabelImageDataset(val3, img_dir=\"data/planet\\planet/train-jpg\", transform=transform)\n",
    "val3_dataset = Subset(val3_dataset, list(range(len(val3))))\n",
    "val4_dataset = MultiLabelImageDataset(val4, img_dir=\"data/planet\\planet/train-jpg\", transform=transform)\n",
    "val4_dataset = Subset(val4_dataset, list(range(len(val4))))\n",
    "val5_dataset = MultiLabelImageDataset(val5, img_dir=\"data/planet\\planet/train-jpg\", transform=transform)\n",
    "val5_dataset = Subset(val5_dataset, list(range(len(val5))))\n",
    "\n",
    "\n",
    "#train_dataset = Subset(dataset, train_idx)\n",
    "#test_dataset = Subset(dataset, test_idx)\n",
    "\n",
    "print(len(train_idx))\n",
    "print(len(test_idx))\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "val1_dataloader = DataLoader(val1_dataset, batch_size=batch_size, shuffle=False)\n",
    "val2_dataloader = DataLoader(val2_dataset, batch_size=batch_size, shuffle=False)\n",
    "val3_dataloader = DataLoader(val3_dataset, batch_size=batch_size, shuffle=False)\n",
    "val4_dataloader = DataLoader(val4_dataset, batch_size=batch_size, shuffle=False)\n",
    "val5_dataloader = DataLoader(val5_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, num_classes, ensemble_type='weighted'):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        \n",
    "        # initialize individual models\n",
    "        self.resnet = ResNetClassifier(num_classes)\n",
    "        self.effnet = ResNetClassifier(num_classes)\n",
    "        self.resnet2 = ResNetClassifier(num_classes)\n",
    "\n",
    "        self.resnet.load_state_dict(torch.load(\"LP_oversampled_ResNet50_0.5epochs_1e-4_ADAM.pth\", weights_only=True))\n",
    "        self.effnet.load_state_dict(torch.load(\"ML_oversampled_augmented_ResNet50_5epochs_1e-4_ADAM.pth\", weights_only=True))\n",
    "        self.resnet2.load_state_dict(torch.load(\"LP_oversampled_ResNet50_0.5epochs_1e-4_ADAM.pth\", weights_only=True))\n",
    "        \n",
    "        # ensemble type\n",
    "        self.ensemble_type = ensemble_type\n",
    "        \n",
    "        # weighted averaging - initialised to 1/3 each\n",
    "        if ensemble_type == 'weighted':\n",
    "            self.weights = nn.Parameter(torch.ones(3) / 3)\n",
    "\n",
    "        # parameters for shepard's rule\n",
    "        self.a = 1.0\n",
    "        self.b = 1.0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # get predictions from each model\n",
    "        resnet_out = self.resnet(x)\n",
    "        effnet_out = self.effnet(x)\n",
    "        resnet2_out = self.resnet2(x)\n",
    "        \n",
    "        # ensemble strategies\n",
    "        if self.ensemble_type == 'voting':\n",
    "            # soft voting - average of predictions\n",
    "            return (resnet_out + effnet_out + resnet2_out) / 3\n",
    "        \n",
    "        elif self.ensemble_type == 'weighted':\n",
    "            # weighted average of predictions\n",
    "            # normalize weights to sum to 1\n",
    "            normalized_weights = nn.functional.softmax(self.weights, dim=0)\n",
    "            \n",
    "            weighted_out = (\n",
    "                normalized_weights[0] * resnet_out + \n",
    "                normalized_weights[1] * effnet_out + \n",
    "                normalized_weights[2] * resnet2_out\n",
    "            )\n",
    "            return weighted_out\n",
    "\n",
    "        elif self.ensemble_type == 'dudani':\n",
    "            # dudani's rule weights\n",
    "            distances = torch.stack([\n",
    "                -torch.max(torch.sigmoid(resnet_out), dim=1)[0],\n",
    "                -torch.max(torch.sigmoid(effnet_out), dim=1)[0],\n",
    "                -torch.max(torch.sigmoid(resnet2_out), dim=1)[0]\n",
    "            ], dim=1) \n",
    "            \n",
    "            d1, _ = torch.min(distances, dim=1, keepdim=True)\n",
    "            dq, _ = torch.max(distances, dim=1, keepdim=True)\n",
    "            \n",
    "            diff = dq - d1\n",
    "            diff[diff == 0] = 1e-10\n",
    "            \n",
    "            dudani_weights = (dq - distances) / diff\n",
    "            dudani_weights = dudani_weights / dudani_weights.sum(dim=1, keepdim=True)\n",
    "\n",
    "            weighted_out = (\n",
    "                dudani_weights[:, 0].unsqueeze(1) * resnet_out +\n",
    "                dudani_weights[:, 1].unsqueeze(1) * effnet_out +\n",
    "                dudani_weights[:, 2].unsqueeze(1) * resnet2_out\n",
    "            )\n",
    "            return weighted_out\n",
    "\n",
    "        elif self.ensemble_type == 'shepard':\n",
    "            distances = torch.stack([\n",
    "                -torch.max(torch.sigmoid(resnet_out), dim=1)[0],\n",
    "                -torch.max(torch.sigmoid(effnet_out), dim=1)[0],\n",
    "                -torch.max(torch.sigmoid(resnet2_out), dim=1)[0]\n",
    "            ], dim=1)\n",
    "\n",
    "            shepard_weights = torch.exp(-self.a * torch.abs(distances) ** self.b)\n",
    "            shepard_weights = shepard_weights / shepard_weights.sum(dim=1, keepdim=True)\n",
    "\n",
    "            weighted_out = (\n",
    "                shepard_weights[:, 0].unsqueeze(1) * resnet_out +\n",
    "                shepard_weights[:, 1].unsqueeze(1) * effnet_out +\n",
    "                shepard_weights[:, 2].unsqueeze(1) * resnet2_out\n",
    "            )\n",
    "            return weighted_out\n",
    "        \n",
    "        else:\n",
    "            return (resnet_out + effnet_out + resnet2_out) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xxpur\\AppData\\Local\\Temp\\ipykernel_30036\\2592304351.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test.load_state_dict(torch.load(\"LP_OS_ML_OS_ResNet50.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EnsembleModel(\n",
       "  (resnet): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=17, bias=True)\n",
       "  )\n",
       "  (effnet): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=17, bias=True)\n",
       "  )\n",
       "  (resnet2): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=17, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test = ResNetClassifier(17)\n",
    "test = EnsembleModel(17,'weighted')\n",
    "test.to(device)\n",
    "test.load_state_dict(torch.load(\"LP_OS_ML_OS_ResNet50.pth\"))\n",
    "test.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_val_loop(dataloader, model, threshold):\n",
    "    model.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    f2 = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "\n",
    "            # calculate f2 score\n",
    "            pred_tags = torch.sigmoid(pred).cpu().numpy() > threshold #0.24\n",
    "            true_tags = y.cpu().numpy()\n",
    "            #f2 += fbeta_score(true_tags, pred_tags, beta=2, average='micro')\n",
    "            print(fbeta_score(true_tags, pred_tags, beta=2, average='micro'))\n",
    "\n",
    "    f2 /= num_batches\n",
    "    return f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9900990099009901\n",
      "0.9895833333333334\n",
      "0.9813084112149532\n",
      "0.9917355371900827\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m threshold_list \u001b[38;5;241m=\u001b[39m [i\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m100\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m41\u001b[39m)]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m threshold \u001b[38;5;129;01min\u001b[39;00m threshold_list:\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(threshold, \u001b[43mcustom_val_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval1_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m#print(threshold, custom_val_loop(val1_dataloader, test, threshold))\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# print(threshold, custom_val_loop(val2_dataloader, test, threshold))\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# print(threshold, custom_val_loop(val3_dataloader, test, threshold))\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# print(threshold, custom_val_loop(val4_dataloader, test, threshold))\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# print(threshold, custom_val_loop(val5_dataloader, test, threshold))\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[30], line 7\u001b[0m, in \u001b[0;36mcustom_val_loop\u001b[1;34m(dataloader, model, threshold)\u001b[0m\n\u001b[0;32m      4\u001b[0m f2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m      8\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m         pred \u001b[38;5;241m=\u001b[39m model(X)\n",
      "File \u001b[1;32md:\\Anaconda_Environments\\For_ML\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\Anaconda_Environments\\For_ML\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Anaconda_Environments\\For_ML\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32md:\\Anaconda_Environments\\For_ML\\lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32md:\\Anaconda_Environments\\For_ML\\lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m, in \u001b[0;36mMultiLabelImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     21\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 24\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m tags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m     28\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(tag_to_idx))\n",
      "File \u001b[1;32md:\\Anaconda_Environments\\For_ML\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32md:\\Anaconda_Environments\\For_ML\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda_Environments\\For_ML\\lib\\site-packages\\torchvision\\transforms\\functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], F_pil\u001b[38;5;241m.\u001b[39mget_image_num_channels(pic))\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "threshold_list = [i/100 for i in range(10, 41)]\n",
    "\n",
    "for threshold in threshold_list:\n",
    "    print(threshold, custom_val_loop(val1_dataloader, test, threshold))\n",
    "    #print(threshold, custom_val_loop(val1_dataloader, test, threshold))\n",
    "    # print(threshold, custom_val_loop(val2_dataloader, test, threshold))\n",
    "    # print(threshold, custom_val_loop(val3_dataloader, test, threshold))\n",
    "    # print(threshold, custom_val_loop(val4_dataloader, test, threshold))\n",
    "    # print(threshold, custom_val_loop(val5_dataloader, test, threshold))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fbeta_score_by_class(pred_tags, true_tags, beta, average):\n",
    "    \"\"\"\n",
    "    Calculate the F2 score for each class.\n",
    "    pred_tags: numpy array of shape (batch_size, num_classes)\n",
    "    true_tags: numpy array of shape (batch_size, num_classes)\n",
    "\n",
    "    Returns:\n",
    "    f2_list: numpy array of shape (num_classes,)\n",
    "    \"\"\"\n",
    "    pred_tags = pred_tags.T\n",
    "    true_tags = true_tags.T\n",
    "\n",
    "    f2_list = np.zeros(pred_tags.shape[0])\n",
    "\n",
    "    for i in range(pred_tags.shape[0]):\n",
    "        pred_class = pred_tags[i]  # shape (batch_size,)\n",
    "        true_class = true_tags[i]  # shape (batch_size,)\n",
    "        f2_list[i] = fbeta_score(true_class, pred_class, beta=beta, average=average)\n",
    "\n",
    "    return f2_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_threshold_indiv_class(dataloader, model, threshold_list, num_classes):\n",
    "  model.eval()\n",
    "  size = len(dataloader.dataset)\n",
    "  num_batches = len(dataloader)\n",
    "  f2_list = np.zeros((len(threshold_list),num_classes))\n",
    "  best_threshold_per_class = np.zeros(num_classes)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for X, y in dataloader:\n",
    "      X, y = X.to(device), y.to(device)\n",
    "      pred = model(X)\n",
    "      true_tags = y.cpu().numpy()\n",
    "\n",
    "      pred = torch.sigmoid(pred).cpu().numpy()\n",
    "      #print(pred.shape)\n",
    "\n",
    "      for i in range(len(threshold_list)):\n",
    "        pred_tags = pred > threshold_list[i]\n",
    "        #print(pred_tags.shape)\n",
    "        f2_list[i] += fbeta_score_by_class(pred_tags, true_tags, beta=2, average=\"micro\")\n",
    "\n",
    "  f2_list /= num_batches\n",
    "\n",
    "  f2_list = f2_list.T   # f2_list: shape of (num_classes x len(threshold_list))\n",
    "\n",
    "  for i in range(f2_list.shape[0]):\n",
    "    idx = np.argmax(f2_list[i])\n",
    "    print(f\"Best threshold for class {i}: {threshold_list[idx]}, f2: {f2_list[i][idx]}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "    best_threshold_per_class[i] = threshold_list[idx]\n",
    "  print(f2_list)\n",
    "\n",
    "  return f2_list, best_threshold_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold for class 0: 0.39, f2: 0.9888833992094862\n",
      "----------------------------------------------\n",
      "Best threshold for class 1: 0.06, f2: 0.9995059288537549\n",
      "----------------------------------------------\n",
      "Best threshold for class 2: 0.11, f2: 0.9992588932806324\n",
      "----------------------------------------------\n",
      "Best threshold for class 3: 0.14, f2: 0.9990118577075099\n",
      "----------------------------------------------\n",
      "Best threshold for class 4: 0.02, f2: 0.9997529644268774\n",
      "----------------------------------------------\n",
      "Best threshold for class 5: 0.39, f2: 0.9933300395256917\n",
      "----------------------------------------------\n",
      "Best threshold for class 6: 0.31, f2: 0.9940711462450593\n",
      "----------------------------------------------\n",
      "Best threshold for class 7: 0.01, f2: 1.0\n",
      "----------------------------------------------\n",
      "Best threshold for class 8: 0.3, f2: 0.9923418972332015\n",
      "----------------------------------------------\n",
      "Best threshold for class 9: 0.39, f2: 0.995306324110672\n",
      "----------------------------------------------\n",
      "Best threshold for class 10: 0.35, f2: 0.9950592885375494\n",
      "----------------------------------------------\n",
      "Best threshold for class 11: 0.24, f2: 0.9962944664031621\n",
      "----------------------------------------------\n",
      "Best threshold for class 12: 0.37, f2: 0.9938241106719368\n",
      "----------------------------------------------\n",
      "Best threshold for class 13: 0.33, f2: 0.9935770750988142\n",
      "----------------------------------------------\n",
      "Best threshold for class 14: 0.33, f2: 0.9995059288537549\n",
      "----------------------------------------------\n",
      "Best threshold for class 15: 0.05, f2: 0.9995059288537549\n",
      "----------------------------------------------\n",
      "Best threshold for class 16: 0.37, f2: 0.9893774703557312\n",
      "----------------------------------------------\n",
      "[[0.85770751 0.92267787 0.94664032 0.95750988 0.96492095 0.96887352\n",
      "  0.9701087  0.97480237 0.97677866 0.9777668  0.97999012 0.98097826\n",
      "  0.98246047 0.98344862 0.98394269 0.98567194 0.98567194 0.98591897\n",
      "  0.98666008 0.98690711 0.98740119 0.98764822 0.98764822 0.98740119\n",
      "  0.98740119 0.98764822 0.98764822 0.98789526 0.98764822 0.98740119\n",
      "  0.98764822 0.98838933 0.98838933 0.98863636 0.98863636 0.98863636\n",
      "  0.98863636 0.98838933 0.9888834  0.9888834 ]\n",
      " [0.99827075 0.99876482 0.99901186 0.99901186 0.99901186 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593]\n",
      " [0.9854249  0.99184783 0.9965415  0.99703557 0.99851779 0.99851779\n",
      "  0.99876482 0.99876482 0.99876482 0.99876482 0.99925889 0.99925889\n",
      "  0.99925889 0.99925889 0.99925889 0.99901186 0.99901186 0.99901186\n",
      "  0.99901186 0.99901186 0.99901186 0.99901186 0.99901186 0.99901186\n",
      "  0.99901186 0.99901186 0.99901186 0.99901186 0.99901186 0.99901186\n",
      "  0.99901186 0.99901186 0.99901186 0.99901186 0.99901186 0.99901186\n",
      "  0.99901186 0.99901186 0.99901186 0.99901186]\n",
      " [0.99357708 0.9965415  0.99827075 0.99827075 0.99827075 0.99827075\n",
      "  0.99827075 0.99827075 0.99851779 0.99851779 0.99851779 0.99876482\n",
      "  0.99876482 0.99901186 0.99901186 0.99901186 0.99901186 0.99901186\n",
      "  0.99901186 0.99901186 0.99901186 0.99901186 0.99901186 0.99901186\n",
      "  0.99901186 0.99901186 0.99901186 0.99901186 0.99901186 0.99901186\n",
      "  0.99901186 0.99901186 0.99901186 0.99901186 0.99901186 0.99901186\n",
      "  0.99901186 0.99901186 0.99901186 0.99901186]\n",
      " [0.99950593 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296]\n",
      " [0.95874506 0.97332016 0.97801383 0.97974308 0.98147233 0.98270751\n",
      "  0.98394269 0.98418972 0.98493083 0.98591897 0.98616601 0.98666008\n",
      "  0.98715415 0.98789526 0.98814229 0.9888834  0.9888834  0.98913043\n",
      "  0.98937747 0.98987154 0.99011858 0.98987154 0.99085968 0.99135375\n",
      "  0.99160079 0.99160079 0.99184783 0.99209486 0.99209486 0.9923419\n",
      "  0.9923419  0.9923419  0.99258893 0.99283597 0.99283597 0.993083\n",
      "  0.993083   0.99283597 0.99333004 0.99333004]\n",
      " [0.97504941 0.97974308 0.98221344 0.98394269 0.98567194 0.98690711\n",
      "  0.98814229 0.98814229 0.98838933 0.98913043 0.99011858 0.99135375\n",
      "  0.99135375 0.99135375 0.99184783 0.99283597 0.99258893 0.99333004\n",
      "  0.99357708 0.99357708 0.99333004 0.99333004 0.99333004 0.99382411\n",
      "  0.99382411 0.99357708 0.99357708 0.99357708 0.99382411 0.99382411\n",
      "  0.99407115 0.99407115 0.99357708 0.99333004 0.99333004 0.993083\n",
      "  0.993083   0.99283597 0.993083   0.993083  ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.88389328 0.92416008 0.94367589 0.95454545 0.96096838 0.96763834\n",
      "  0.97134387 0.97480237 0.97702569 0.98023715 0.98246047 0.98443676\n",
      "  0.98493083 0.98666008 0.9888834  0.98863636 0.9888834  0.98863636\n",
      "  0.98913043 0.98962451 0.98987154 0.99036561 0.99110672 0.99160079\n",
      "  0.99160079 0.99160079 0.99184783 0.99184783 0.99209486 0.9923419\n",
      "  0.9923419  0.99209486 0.9923419  0.99209486 0.9923419  0.99209486\n",
      "  0.9923419  0.9923419  0.9923419  0.9923419 ]\n",
      " [0.94935771 0.96714427 0.97826087 0.98270751 0.98616601 0.9888834\n",
      "  0.99061265 0.99160079 0.9923419  0.99283597 0.99357708 0.99357708\n",
      "  0.99407115 0.99456522 0.99431818 0.99431818 0.99431818 0.99431818\n",
      "  0.99431818 0.99431818 0.99456522 0.99431818 0.99431818 0.99456522\n",
      "  0.99456522 0.99431818 0.99407115 0.99431818 0.99456522 0.99456522\n",
      "  0.99431818 0.99431818 0.99456522 0.99456522 0.99456522 0.99481225\n",
      "  0.99481225 0.99505929 0.99530632 0.99530632]\n",
      " [0.93898221 0.95503953 0.96442688 0.96986166 0.97332016 0.97727273\n",
      "  0.97924901 0.9819664  0.98344862 0.98394269 0.9854249  0.98641304\n",
      "  0.98764822 0.98764822 0.98863636 0.98962451 0.98987154 0.99011858\n",
      "  0.99036561 0.99160079 0.9923419  0.9923419  0.99258893 0.99283597\n",
      "  0.993083   0.99357708 0.99333004 0.99333004 0.99431818 0.99456522\n",
      "  0.99456522 0.99481225 0.99481225 0.99481225 0.99505929 0.99481225\n",
      "  0.99481225 0.99505929 0.99505929 0.99481225]\n",
      " [0.96467391 0.97949605 0.98690711 0.9888834  0.99036561 0.99160079\n",
      "  0.99258893 0.993083   0.99382411 0.99407115 0.99456522 0.99505929\n",
      "  0.99505929 0.99505929 0.99530632 0.99555336 0.99555336 0.99555336\n",
      "  0.99555336 0.9958004  0.99604743 0.99604743 0.99604743 0.99629447\n",
      "  0.99629447 0.99629447 0.99629447 0.99629447 0.99604743 0.99604743\n",
      "  0.99604743 0.99604743 0.99604743 0.99604743 0.99604743 0.9958004\n",
      "  0.99555336 0.9958004  0.9958004  0.9958004 ]\n",
      " [0.94812253 0.95578063 0.96072134 0.96590909 0.96862648 0.97257905\n",
      "  0.97603755 0.97727273 0.97974308 0.9812253  0.98295455 0.98418972\n",
      "  0.98591897 0.98764822 0.98937747 0.99011858 0.99061265 0.99085968\n",
      "  0.99110672 0.99135375 0.99160079 0.99135375 0.99135375 0.99135375\n",
      "  0.99184783 0.99184783 0.9923419  0.99258893 0.99258893 0.99258893\n",
      "  0.99283597 0.993083   0.99333004 0.99333004 0.99333004 0.99357708\n",
      "  0.99382411 0.99382411 0.99382411 0.99382411]\n",
      " [0.91724308 0.94886364 0.9631917  0.97257905 0.97850791 0.98097826\n",
      "  0.98394269 0.98616601 0.98789526 0.99011858 0.99110672 0.99110672\n",
      "  0.99061265 0.99036561 0.99085968 0.99110672 0.99135375 0.99160079\n",
      "  0.99160079 0.99160079 0.99209486 0.99258893 0.9923419  0.99258893\n",
      "  0.9923419  0.99258893 0.99258893 0.99258893 0.99258893 0.99283597\n",
      "  0.99283597 0.99333004 0.99357708 0.993083   0.99333004 0.99333004\n",
      "  0.993083   0.993083   0.993083   0.99283597]\n",
      " [0.99851779 0.99876482 0.99876482 0.99925889 0.99925889 0.99925889\n",
      "  0.99925889 0.99925889 0.99925889 0.99925889 0.99925889 0.99925889\n",
      "  0.99925889 0.99925889 0.99925889 0.99925889 0.99925889 0.99925889\n",
      "  0.99925889 0.99925889 0.99925889 0.99925889 0.99925889 0.99925889\n",
      "  0.99925889 0.99925889 0.99925889 0.99925889 0.99925889 0.99925889\n",
      "  0.99925889 0.99925889 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593]\n",
      " [0.99925889 0.99925889 0.99925889 0.99925889 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593]\n",
      " [0.87277668 0.930583   0.94935771 0.96047431 0.96590909 0.97134387\n",
      "  0.97356719 0.97480237 0.97603755 0.97751976 0.97850791 0.98048419\n",
      "  0.9812253  0.98295455 0.98443676 0.98418972 0.98493083 0.98517787\n",
      "  0.98567194 0.98567194 0.98641304 0.98666008 0.98715415 0.98715415\n",
      "  0.98789526 0.98814229 0.98838933 0.98838933 0.98863636 0.98863636\n",
      "  0.9888834  0.9888834  0.98913043 0.98913043 0.98863636 0.98913043\n",
      "  0.98937747 0.9888834  0.98863636 0.98863636]]\n",
      "Best threshold for class 0: 0.37, f2: 0.9881422924901185\n",
      "----------------------------------------------\n",
      "Best threshold for class 1: 0.04, f2: 1.0\n",
      "----------------------------------------------\n",
      "Best threshold for class 2: 0.25, f2: 0.9985177865612648\n",
      "----------------------------------------------\n",
      "Best threshold for class 3: 0.2, f2: 0.9990118577075099\n",
      "----------------------------------------------\n",
      "Best threshold for class 4: 0.01, f2: 0.9997529644268774\n",
      "----------------------------------------------\n",
      "Best threshold for class 5: 0.4, f2: 0.9940711462450593\n",
      "----------------------------------------------\n",
      "Best threshold for class 6: 0.28, f2: 0.995306324110672\n",
      "----------------------------------------------\n",
      "Best threshold for class 7: 0.01, f2: 1.0\n",
      "----------------------------------------------\n",
      "Best threshold for class 8: 0.38, f2: 0.9908596837944664\n",
      "----------------------------------------------\n",
      "Best threshold for class 9: 0.16, f2: 0.9945652173913043\n",
      "----------------------------------------------\n",
      "Best threshold for class 10: 0.32, f2: 0.9943181818181818\n",
      "----------------------------------------------\n",
      "Best threshold for class 11: 0.34, f2: 0.9970355731225297\n",
      "----------------------------------------------\n",
      "Best threshold for class 12: 0.4, f2: 0.995800395256917\n",
      "----------------------------------------------\n",
      "Best threshold for class 13: 0.25, f2: 0.9920948616600791\n",
      "----------------------------------------------\n",
      "Best threshold for class 14: 0.2, f2: 0.9995059288537549\n",
      "----------------------------------------------\n",
      "Best threshold for class 15: 0.06, f2: 0.9997529644268774\n",
      "----------------------------------------------\n",
      "Best threshold for class 16: 0.38, f2: 0.991106719367589\n",
      "----------------------------------------------\n",
      "[[0.86610672 0.93107708 0.95281621 0.95874506 0.96590909 0.97159091\n",
      "  0.97504941 0.97826087 0.97924901 0.98048419 0.98147233 0.98221344\n",
      "  0.98369565 0.98493083 0.98567194 0.98616601 0.98641304 0.98666008\n",
      "  0.98641304 0.98641304 0.98641304 0.98641304 0.98641304 0.98641304\n",
      "  0.98666008 0.98764822 0.98789526 0.98764822 0.98764822 0.98764822\n",
      "  0.98740119 0.98715415 0.98715415 0.98764822 0.98764822 0.98764822\n",
      "  0.98814229 0.98740119 0.98715415 0.98715415]\n",
      " [0.99950593 0.99950593 0.99975296 1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.97924901 0.99036561 0.99382411 0.99481225 0.99678854 0.99703557\n",
      "  0.99703557 0.99752964 0.99777668 0.99777668 0.99777668 0.99752964\n",
      "  0.99777668 0.99777668 0.99777668 0.99777668 0.99802372 0.99802372\n",
      "  0.99802372 0.99827075 0.99827075 0.99827075 0.99827075 0.99827075\n",
      "  0.99851779 0.99851779 0.99851779 0.99851779 0.99851779 0.99851779\n",
      "  0.99851779 0.99851779 0.99851779 0.99851779 0.99851779 0.99851779\n",
      "  0.99851779 0.99851779 0.99851779 0.99851779]\n",
      " [0.99431818 0.9965415  0.99752964 0.99752964 0.99802372 0.99827075\n",
      "  0.99827075 0.99851779 0.99851779 0.99851779 0.99876482 0.99876482\n",
      "  0.99876482 0.99876482 0.99876482 0.99876482 0.99876482 0.99876482\n",
      "  0.99876482 0.99901186 0.99901186 0.99901186 0.99901186 0.99901186\n",
      "  0.99901186 0.99901186 0.99901186 0.99901186 0.99901186 0.99901186\n",
      "  0.99901186 0.99901186 0.99901186 0.99901186 0.99901186 0.99901186\n",
      "  0.99901186 0.99901186 0.99901186 0.99901186]\n",
      " [0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296]\n",
      " [0.96195652 0.97381423 0.97801383 0.98147233 0.98320158 0.98468379\n",
      "  0.98567194 0.98641304 0.98666008 0.98715415 0.98715415 0.98814229\n",
      "  0.98863636 0.9888834  0.98913043 0.98913043 0.98913043 0.98962451\n",
      "  0.99061265 0.99085968 0.99085968 0.99061265 0.99085968 0.99085968\n",
      "  0.99110672 0.99110672 0.99160079 0.99184783 0.99209486 0.99258893\n",
      "  0.993083   0.993083   0.993083   0.99333004 0.99333004 0.99357708\n",
      "  0.99357708 0.99382411 0.99382411 0.99407115]\n",
      " [0.9777668  0.98221344 0.98418972 0.98666008 0.98715415 0.98789526\n",
      "  0.98863636 0.98913043 0.98987154 0.99011858 0.99061265 0.99110672\n",
      "  0.99135375 0.99135375 0.99209486 0.99258893 0.99283597 0.993083\n",
      "  0.993083   0.993083   0.99382411 0.99382411 0.99456522 0.99481225\n",
      "  0.99481225 0.99505929 0.99505929 0.99530632 0.99505929 0.99481225\n",
      "  0.99456522 0.99456522 0.99431818 0.99456522 0.99456522 0.99481225\n",
      "  0.99481225 0.99481225 0.99456522 0.99456522]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.87450593 0.91650198 0.93601779 0.95059289 0.95800395 0.96368577\n",
      "  0.96640316 0.97035573 0.97332016 0.97677866 0.97875494 0.97875494\n",
      "  0.98097826 0.98295455 0.98369565 0.98369565 0.98567194 0.98641304\n",
      "  0.98764822 0.98764822 0.98789526 0.98814229 0.98838933 0.98937747\n",
      "  0.98962451 0.98937747 0.98937747 0.98962451 0.98962451 0.98987154\n",
      "  0.98987154 0.98987154 0.98987154 0.99011858 0.99036561 0.99011858\n",
      "  0.99036561 0.99085968 0.99085968 0.99085968]\n",
      " [0.94664032 0.96763834 0.97949605 0.9854249  0.98764822 0.99061265\n",
      "  0.99110672 0.9923419  0.99283597 0.99333004 0.99357708 0.99382411\n",
      "  0.99357708 0.99382411 0.99407115 0.99456522 0.99431818 0.99431818\n",
      "  0.99456522 0.99456522 0.99407115 0.99382411 0.99382411 0.99382411\n",
      "  0.99382411 0.99382411 0.99382411 0.99382411 0.99382411 0.99357708\n",
      "  0.99357708 0.99357708 0.99357708 0.99357708 0.99333004 0.993083\n",
      "  0.993083   0.993083   0.993083   0.993083  ]\n",
      " [0.94046443 0.95923913 0.9673913  0.97159091 0.97504941 0.9777668\n",
      "  0.98023715 0.98344862 0.98468379 0.98616601 0.98764822 0.9888834\n",
      "  0.98962451 0.99036561 0.99036561 0.99011858 0.98987154 0.98987154\n",
      "  0.99036561 0.99061265 0.99085968 0.99160079 0.99184783 0.99258893\n",
      "  0.99258893 0.99258893 0.9923419  0.99283597 0.99357708 0.99382411\n",
      "  0.99407115 0.99431818 0.99431818 0.99431818 0.99431818 0.99407115\n",
      "  0.99407115 0.99407115 0.99407115 0.99407115]\n",
      " [0.96195652 0.97900198 0.98493083 0.98913043 0.98987154 0.99184783\n",
      "  0.99258893 0.993083   0.99382411 0.99407115 0.99456522 0.99481225\n",
      "  0.99505929 0.99555336 0.99555336 0.99555336 0.99555336 0.9958004\n",
      "  0.9958004  0.9958004  0.9958004  0.9958004  0.99604743 0.99604743\n",
      "  0.9958004  0.99604743 0.99629447 0.99629447 0.9965415  0.99678854\n",
      "  0.99678854 0.99678854 0.99678854 0.99703557 0.99703557 0.99703557\n",
      "  0.99703557 0.99703557 0.99703557 0.99703557]\n",
      " [0.9555336  0.96245059 0.96813241 0.97134387 0.97381423 0.97554348\n",
      "  0.97801383 0.9812253  0.98270751 0.98394269 0.98567194 0.98715415\n",
      "  0.98838933 0.9888834  0.99085968 0.99160079 0.9923419  0.99209486\n",
      "  0.99258893 0.99333004 0.99333004 0.99382411 0.99407115 0.99431818\n",
      "  0.99456522 0.99456522 0.99505929 0.99505929 0.99505929 0.99530632\n",
      "  0.99530632 0.99555336 0.99555336 0.99555336 0.99530632 0.99505929\n",
      "  0.99505929 0.99530632 0.99555336 0.9958004 ]\n",
      " [0.90983202 0.94589921 0.96146245 0.96763834 0.97480237 0.97727273\n",
      "  0.98023715 0.98246047 0.98517787 0.98690711 0.98838933 0.9888834\n",
      "  0.98987154 0.99011858 0.99036561 0.99061265 0.99061265 0.99085968\n",
      "  0.99110672 0.99110672 0.99135375 0.99184783 0.99184783 0.99160079\n",
      "  0.99209486 0.99184783 0.99160079 0.99160079 0.99160079 0.99135375\n",
      "  0.99135375 0.99110672 0.99085968 0.99036561 0.99036561 0.99036561\n",
      "  0.98987154 0.98987154 0.98987154 0.98987154]\n",
      " [0.99703557 0.99827075 0.99876482 0.99901186 0.99925889 0.99925889\n",
      "  0.99925889 0.99925889 0.99925889 0.99925889 0.99925889 0.99925889\n",
      "  0.99925889 0.99925889 0.99925889 0.99925889 0.99925889 0.99925889\n",
      "  0.99925889 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593]\n",
      " [0.99851779 0.99925889 0.99950593 0.99950593 0.99950593 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296]\n",
      " [0.87327075 0.92811265 0.95158103 0.96294466 0.96961462 0.97257905\n",
      "  0.97554348 0.97751976 0.97924901 0.97999012 0.98073123 0.98221344\n",
      "  0.98394269 0.98443676 0.98517787 0.98517787 0.9854249  0.9854249\n",
      "  0.98666008 0.98690711 0.98690711 0.98666008 0.98666008 0.98715415\n",
      "  0.98740119 0.98740119 0.98764822 0.98814229 0.98863636 0.98913043\n",
      "  0.98937747 0.98913043 0.98913043 0.98987154 0.99011858 0.99085968\n",
      "  0.99085968 0.99110672 0.99061265 0.99085968]]\n",
      "Best threshold for class 0: 0.37, f2: 0.9908596837944664\n",
      "----------------------------------------------\n",
      "Best threshold for class 1: 0.05, f2: 1.0\n",
      "----------------------------------------------\n",
      "Best threshold for class 2: 0.33, f2: 0.9987648221343873\n",
      "----------------------------------------------\n",
      "Best threshold for class 3: 0.09, f2: 0.9997529644268774\n",
      "----------------------------------------------\n",
      "Best threshold for class 4: 0.01, f2: 0.9992588932806324\n",
      "----------------------------------------------\n",
      "Best threshold for class 5: 0.39, f2: 0.9933300395256917\n",
      "----------------------------------------------\n",
      "Best threshold for class 6: 0.27, f2: 0.995306324110672\n",
      "----------------------------------------------\n",
      "Best threshold for class 7: 0.03, f2: 1.0\n",
      "----------------------------------------------\n",
      "Best threshold for class 8: 0.38, f2: 0.9923418972332015\n",
      "----------------------------------------------\n",
      "Best threshold for class 9: 0.15, f2: 0.9967885375494071\n",
      "----------------------------------------------\n",
      "Best threshold for class 10: 0.39, f2: 0.995306324110672\n",
      "----------------------------------------------\n",
      "Best threshold for class 11: 0.34, f2: 0.9965415019762845\n",
      "----------------------------------------------\n",
      "Best threshold for class 12: 0.4, f2: 0.9943181818181818\n",
      "----------------------------------------------\n",
      "Best threshold for class 13: 0.26, f2: 0.9930830039525692\n",
      "----------------------------------------------\n",
      "Best threshold for class 14: 0.05, f2: 0.9997529644268774\n",
      "----------------------------------------------\n",
      "Best threshold for class 15: 0.02, f2: 1.0\n",
      "----------------------------------------------\n",
      "Best threshold for class 16: 0.26, f2: 0.9876482213438735\n",
      "----------------------------------------------\n",
      "[[0.86215415 0.92490119 0.94491107 0.95825099 0.96566206 0.9701087\n",
      "  0.97307312 0.97653162 0.9777668  0.97900198 0.98023715 0.9819664\n",
      "  0.98295455 0.98369565 0.98443676 0.98517787 0.9854249  0.98567194\n",
      "  0.98641304 0.98641304 0.98690711 0.98690711 0.98715415 0.98740119\n",
      "  0.98764822 0.98789526 0.98789526 0.98814229 0.98814229 0.98838933\n",
      "  0.9888834  0.98937747 0.98962451 0.99011858 0.99011858 0.99011858\n",
      "  0.99085968 0.99061265 0.99011858 0.98987154]\n",
      " [0.99901186 0.99950593 0.99975296 0.99975296 1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.98073123 0.99061265 0.99456522 0.99505929 0.99604743 0.9965415\n",
      "  0.9965415  0.99703557 0.99703557 0.99703557 0.99777668 0.99802372\n",
      "  0.99802372 0.99802372 0.99802372 0.99827075 0.99827075 0.99827075\n",
      "  0.99827075 0.99827075 0.99827075 0.99827075 0.99851779 0.99851779\n",
      "  0.99851779 0.99851779 0.99851779 0.99851779 0.99851779 0.99851779\n",
      "  0.99851779 0.99851779 0.99876482 0.99876482 0.99876482 0.99876482\n",
      "  0.99876482 0.99876482 0.99876482 0.99876482]\n",
      " [0.99530632 0.99827075 0.99876482 0.99901186 0.99925889 0.99950593\n",
      "  0.99950593 0.99950593 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296]\n",
      " [0.99925889 0.99925889 0.99925889 0.99925889 0.99925889 0.99925889\n",
      "  0.99925889 0.99925889 0.99925889 0.99925889 0.99925889 0.99925889\n",
      "  0.99925889 0.99925889 0.99925889 0.99925889 0.99925889 0.99925889\n",
      "  0.99925889 0.99925889 0.99925889 0.99925889 0.99925889 0.99925889\n",
      "  0.99925889 0.99925889 0.99925889 0.99925889 0.99925889 0.99925889\n",
      "  0.99925889 0.99925889 0.99925889 0.99925889 0.99925889 0.99925889\n",
      "  0.99925889 0.99925889 0.99925889 0.99925889]\n",
      " [0.96269763 0.97504941 0.97900198 0.9819664  0.98295455 0.98493083\n",
      "  0.98567194 0.98666008 0.98690711 0.98764822 0.98764822 0.98789526\n",
      "  0.98814229 0.98863636 0.98863636 0.98913043 0.98962451 0.98987154\n",
      "  0.98987154 0.99036561 0.99061265 0.99085968 0.99110672 0.99135375\n",
      "  0.99160079 0.99209486 0.99209486 0.99258893 0.99258893 0.99258893\n",
      "  0.99258893 0.99283597 0.99283597 0.99283597 0.99258893 0.993083\n",
      "  0.993083   0.99283597 0.99333004 0.99333004]\n",
      " [0.97406126 0.9777668  0.98147233 0.98493083 0.98591897 0.98641304\n",
      "  0.98715415 0.98838933 0.9888834  0.9888834  0.98937747 0.99135375\n",
      "  0.99135375 0.9923419  0.9923419  0.99283597 0.99283597 0.99283597\n",
      "  0.99283597 0.993083   0.99382411 0.99407115 0.99456522 0.99481225\n",
      "  0.99505929 0.99505929 0.99530632 0.99530632 0.99530632 0.99505929\n",
      "  0.99530632 0.99530632 0.99505929 0.99456522 0.99456522 0.99456522\n",
      "  0.99456522 0.99481225 0.99431818 0.99382411]\n",
      " [0.99975296 0.99975296 1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.87376482 0.91576087 0.93799407 0.95331028 0.96047431 0.9673913\n",
      "  0.9708498  0.97381423 0.97801383 0.98023715 0.98171937 0.98320158\n",
      "  0.98394269 0.98443676 0.98591897 0.98690711 0.98789526 0.98789526\n",
      "  0.9888834  0.98913043 0.98913043 0.98937747 0.98962451 0.99036561\n",
      "  0.99110672 0.99085968 0.99110672 0.99085968 0.99135375 0.99160079\n",
      "  0.99184783 0.99160079 0.99135375 0.99135375 0.99135375 0.99160079\n",
      "  0.99184783 0.9923419  0.9923419  0.99209486]\n",
      " [0.94812253 0.97183794 0.98295455 0.98740119 0.99011858 0.99333004\n",
      "  0.99431818 0.99505929 0.99481225 0.99555336 0.9958004  0.99604743\n",
      "  0.99604743 0.99629447 0.99678854 0.99678854 0.99678854 0.99678854\n",
      "  0.9965415  0.9965415  0.9965415  0.9965415  0.9965415  0.9965415\n",
      "  0.9965415  0.9965415  0.9965415  0.99629447 0.99629447 0.99629447\n",
      "  0.99604743 0.99629447 0.99629447 0.99629447 0.99629447 0.99629447\n",
      "  0.99629447 0.99604743 0.99604743 0.99604743]\n",
      " [0.94194664 0.95998024 0.96887352 0.97381423 0.97702569 0.98023715\n",
      "  0.98369565 0.98616601 0.98789526 0.98913043 0.99036561 0.99085968\n",
      "  0.99135375 0.99135375 0.99160079 0.99184783 0.99184783 0.9923419\n",
      "  0.99258893 0.99258893 0.99382411 0.99382411 0.99382411 0.99382411\n",
      "  0.99382411 0.99407115 0.99382411 0.99407115 0.99431818 0.99431818\n",
      "  0.99431818 0.99431818 0.99431818 0.99481225 0.99481225 0.99481225\n",
      "  0.99481225 0.99505929 0.99530632 0.99505929]\n",
      " [0.96121542 0.97826087 0.98418972 0.9888834  0.98937747 0.99011858\n",
      "  0.99135375 0.99258893 0.99283597 0.99283597 0.99382411 0.99456522\n",
      "  0.99530632 0.99530632 0.99505929 0.99530632 0.99530632 0.99555336\n",
      "  0.99555336 0.9958004  0.99555336 0.9958004  0.9958004  0.9958004\n",
      "  0.9958004  0.9958004  0.99604743 0.99629447 0.99629447 0.99629447\n",
      "  0.99629447 0.99629447 0.99629447 0.9965415  0.9965415  0.9965415\n",
      "  0.9965415  0.9965415  0.9965415  0.9965415 ]\n",
      " [0.95083992 0.95800395 0.96245059 0.96566206 0.96936759 0.97282609\n",
      "  0.97579051 0.97702569 0.97924901 0.98171937 0.98394269 0.98468379\n",
      "  0.9854249  0.98616601 0.98715415 0.98838933 0.98937747 0.99011858\n",
      "  0.99011858 0.99036561 0.99061265 0.99085968 0.99110672 0.99110672\n",
      "  0.99110672 0.99110672 0.99135375 0.99184783 0.99209486 0.99209486\n",
      "  0.9923419  0.9923419  0.99258893 0.993083   0.99283597 0.99333004\n",
      "  0.99382411 0.99382411 0.99382411 0.99431818]\n",
      " [0.9194664  0.95306324 0.9666502  0.97282609 0.97751976 0.98048419\n",
      "  0.98246047 0.98443676 0.98616601 0.98740119 0.98913043 0.98987154\n",
      "  0.98962451 0.99011858 0.99061265 0.99110672 0.99135375 0.99160079\n",
      "  0.99184783 0.9923419  0.9923419  0.99258893 0.99258893 0.99258893\n",
      "  0.99283597 0.993083   0.99283597 0.99283597 0.99283597 0.99258893\n",
      "  0.99209486 0.99184783 0.99209486 0.99209486 0.99209486 0.99184783\n",
      "  0.99184783 0.99184783 0.99184783 0.99160079]\n",
      " [0.99777668 0.99851779 0.99901186 0.99950593 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99925889 0.99925889 0.99925889 0.99925889\n",
      "  0.99925889 0.99925889 0.99925889 0.99925889]\n",
      " [0.99876482 1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.87598814 0.93107708 0.94738142 0.95750988 0.96516798 0.97035573\n",
      "  0.97356719 0.97579051 0.97727273 0.97924901 0.97999012 0.9812253\n",
      "  0.9819664  0.98295455 0.98369565 0.98394269 0.98418972 0.98468379\n",
      "  0.98517787 0.98567194 0.98616601 0.98616601 0.98666008 0.98666008\n",
      "  0.98740119 0.98764822 0.98764822 0.98740119 0.98764822 0.98740119\n",
      "  0.98740119 0.98715415 0.98740119 0.98740119 0.98740119 0.98740119\n",
      "  0.98740119 0.98740119 0.98764822 0.98764822]]\n",
      "Best threshold for class 0: 0.38, f2: 0.9886363636363636\n",
      "----------------------------------------------\n",
      "Best threshold for class 1: 0.06, f2: 0.9995059288537549\n",
      "----------------------------------------------\n",
      "Best threshold for class 2: 0.35, f2: 0.9990118577075099\n",
      "----------------------------------------------\n",
      "Best threshold for class 3: 0.36, f2: 0.9992588932806324\n",
      "----------------------------------------------\n",
      "Best threshold for class 4: 0.05, f2: 0.9997529644268774\n",
      "----------------------------------------------\n",
      "Best threshold for class 5: 0.36, f2: 0.9923418972332015\n",
      "----------------------------------------------\n",
      "Best threshold for class 6: 0.28, f2: 0.9950592885375494\n",
      "----------------------------------------------\n",
      "Best threshold for class 7: 0.14, f2: 1.0\n",
      "----------------------------------------------\n",
      "Best threshold for class 8: 0.35, f2: 0.9918478260869565\n",
      "----------------------------------------------\n",
      "Best threshold for class 9: 0.26, f2: 0.9955533596837944\n",
      "----------------------------------------------\n",
      "Best threshold for class 10: 0.38, f2: 0.9955533596837944\n",
      "----------------------------------------------\n",
      "Best threshold for class 11: 0.27, f2: 0.9980237154150198\n",
      "----------------------------------------------\n",
      "Best threshold for class 12: 0.4, f2: 0.995306324110672\n",
      "----------------------------------------------\n",
      "Best threshold for class 13: 0.25, f2: 0.9920948616600791\n",
      "----------------------------------------------\n",
      "Best threshold for class 14: 0.04, f2: 1.0\n",
      "----------------------------------------------\n",
      "Best threshold for class 15: 0.03, f2: 0.9995059288537549\n",
      "----------------------------------------------\n",
      "Best threshold for class 16: 0.32, f2: 0.9893774703557312\n",
      "----------------------------------------------\n",
      "[[0.85918972 0.91724308 0.93972332 0.95108696 0.96121542 0.96689723\n",
      "  0.96961462 0.97257905 0.9743083  0.97628458 0.97702569 0.97801383\n",
      "  0.97900198 0.98023715 0.9812253  0.98147233 0.98270751 0.98270751\n",
      "  0.98344862 0.98394269 0.98394269 0.98443676 0.98517787 0.98517787\n",
      "  0.98567194 0.98567194 0.98591897 0.98591897 0.98616601 0.98616601\n",
      "  0.98666008 0.98740119 0.98740119 0.98789526 0.98838933 0.98838933\n",
      "  0.98838933 0.98863636 0.98838933 0.98838933]\n",
      " [0.99802372 0.99876482 0.99925889 0.99925889 0.99925889 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593]\n",
      " [0.97579051 0.98764822 0.99135375 0.993083   0.99505929 0.9958004\n",
      "  0.99629447 0.99728261 0.99728261 0.99728261 0.99752964 0.99777668\n",
      "  0.99777668 0.99777668 0.99802372 0.99802372 0.99827075 0.99827075\n",
      "  0.99827075 0.99827075 0.99827075 0.99827075 0.99851779 0.99851779\n",
      "  0.99851779 0.99851779 0.99851779 0.99851779 0.99851779 0.99851779\n",
      "  0.99851779 0.99851779 0.99876482 0.99876482 0.99901186 0.99901186\n",
      "  0.99901186 0.99901186 0.99901186 0.99901186]\n",
      " [0.99407115 0.99802372 0.99851779 0.99851779 0.99876482 0.99901186\n",
      "  0.99901186 0.99901186 0.99901186 0.99901186 0.99901186 0.99901186\n",
      "  0.99901186 0.99901186 0.99901186 0.99901186 0.99901186 0.99901186\n",
      "  0.99901186 0.99901186 0.99901186 0.99901186 0.99901186 0.99901186\n",
      "  0.99901186 0.99901186 0.99901186 0.99901186 0.99901186 0.99901186\n",
      "  0.99901186 0.99901186 0.99901186 0.99901186 0.99901186 0.99925889\n",
      "  0.99925889 0.99901186 0.99901186 0.99901186]\n",
      " [0.99925889 0.99950593 0.99950593 0.99950593 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296]\n",
      " [0.96343874 0.97183794 0.97628458 0.97949605 0.98023715 0.98246047\n",
      "  0.98443676 0.98591897 0.98666008 0.98764822 0.98838933 0.98863636\n",
      "  0.98838933 0.9888834  0.98913043 0.98913043 0.98937747 0.98987154\n",
      "  0.98987154 0.99061265 0.99061265 0.99061265 0.99085968 0.99135375\n",
      "  0.99135375 0.99184783 0.99209486 0.99209486 0.99209486 0.99209486\n",
      "  0.99209486 0.99209486 0.99209486 0.99209486 0.99209486 0.9923419\n",
      "  0.9923419  0.9923419  0.99209486 0.99209486]\n",
      " [0.97579051 0.98147233 0.98443676 0.98641304 0.98740119 0.98863636\n",
      "  0.99011858 0.99135375 0.99135375 0.99160079 0.99258893 0.993083\n",
      "  0.99357708 0.99357708 0.99357708 0.99357708 0.99382411 0.99357708\n",
      "  0.99357708 0.99357708 0.99382411 0.99382411 0.99382411 0.99431818\n",
      "  0.99431818 0.99431818 0.99456522 0.99505929 0.99456522 0.99481225\n",
      "  0.99481225 0.99481225 0.99481225 0.99481225 0.99481225 0.99481225\n",
      "  0.99481225 0.99456522 0.99431818 0.99407115]\n",
      " [0.99950593 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.88117589 0.92564229 0.944417   0.95899209 0.96368577 0.96986166\n",
      "  0.97381423 0.97677866 0.97999012 0.9819664  0.98320158 0.98517787\n",
      "  0.98641304 0.98764822 0.98863636 0.98962451 0.99036561 0.99011858\n",
      "  0.99011858 0.99061265 0.99011858 0.99085968 0.99085968 0.99085968\n",
      "  0.99085968 0.99135375 0.99135375 0.99085968 0.99135375 0.99110672\n",
      "  0.99110672 0.99135375 0.99135375 0.99160079 0.99184783 0.99184783\n",
      "  0.99160079 0.99184783 0.99160079 0.99135375]\n",
      " [0.94071146 0.9666502  0.97751976 0.98171937 0.98715415 0.98913043\n",
      "  0.98987154 0.99160079 0.99209486 0.9923419  0.99357708 0.99357708\n",
      "  0.99382411 0.99407115 0.99481225 0.99481225 0.99456522 0.99481225\n",
      "  0.99456522 0.99456522 0.99481225 0.99481225 0.99481225 0.99505929\n",
      "  0.99530632 0.99555336 0.99530632 0.99555336 0.99530632 0.99555336\n",
      "  0.99555336 0.99530632 0.99555336 0.99555336 0.99555336 0.99555336\n",
      "  0.99555336 0.99555336 0.99555336 0.99555336]\n",
      " [0.93675889 0.95652174 0.96714427 0.97208498 0.97529644 0.98023715\n",
      "  0.9819664  0.98344862 0.98468379 0.98641304 0.98789526 0.98814229\n",
      "  0.98838933 0.98937747 0.98987154 0.99036561 0.99160079 0.99184783\n",
      "  0.99209486 0.99209486 0.99209486 0.9923419  0.993083   0.993083\n",
      "  0.993083   0.99357708 0.99431818 0.99456522 0.99481225 0.99505929\n",
      "  0.99505929 0.99530632 0.99530632 0.99530632 0.99530632 0.99530632\n",
      "  0.99530632 0.99555336 0.99555336 0.99555336]\n",
      " [0.96763834 0.9819664  0.98740119 0.99085968 0.99184783 0.99258893\n",
      "  0.99357708 0.99407115 0.99456522 0.99530632 0.99604743 0.9965415\n",
      "  0.9965415  0.99703557 0.99728261 0.99728261 0.99728261 0.99728261\n",
      "  0.99728261 0.99728261 0.99752964 0.99728261 0.99752964 0.99752964\n",
      "  0.99752964 0.99752964 0.99802372 0.99802372 0.99802372 0.99802372\n",
      "  0.99802372 0.99802372 0.99802372 0.99802372 0.99802372 0.99802372\n",
      "  0.99802372 0.99802372 0.99802372 0.99802372]\n",
      " [0.95133399 0.95726285 0.96047431 0.96467391 0.96912055 0.97282609\n",
      "  0.97504941 0.97702569 0.97974308 0.98221344 0.98369565 0.98443676\n",
      "  0.98567194 0.98715415 0.98764822 0.98962451 0.99061265 0.99110672\n",
      "  0.99135375 0.99209486 0.99209486 0.99333004 0.99333004 0.99357708\n",
      "  0.99357708 0.99382411 0.99431818 0.99431818 0.99456522 0.99431818\n",
      "  0.99456522 0.99456522 0.99456522 0.99456522 0.99456522 0.99431818\n",
      "  0.99481225 0.99481225 0.99505929 0.99530632]\n",
      " [0.91551383 0.95059289 0.96590909 0.97603755 0.98097826 0.98369565\n",
      "  0.9854249  0.98740119 0.98764822 0.9888834  0.98913043 0.98913043\n",
      "  0.98913043 0.98937747 0.98962451 0.98987154 0.98987154 0.98987154\n",
      "  0.99011858 0.99036561 0.99085968 0.99110672 0.99160079 0.99184783\n",
      "  0.99209486 0.99209486 0.99184783 0.99184783 0.99209486 0.99184783\n",
      "  0.99160079 0.99160079 0.99184783 0.99135375 0.99160079 0.99160079\n",
      "  0.99160079 0.99135375 0.99135375 0.99135375]\n",
      " [0.99876482 0.99925889 0.99950593 1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296]\n",
      " [0.99777668 0.99925889 0.99950593 0.99950593 0.99925889 0.99925889\n",
      "  0.99925889 0.99925889 0.99925889 0.99925889 0.99925889 0.99925889\n",
      "  0.99925889 0.99925889 0.99925889 0.99925889 0.99925889 0.99925889\n",
      "  0.99925889 0.99925889 0.99925889 0.99925889 0.99925889 0.99925889\n",
      "  0.99925889 0.99925889 0.99925889 0.99925889 0.99925889 0.99925889\n",
      "  0.99925889 0.99925889 0.99925889 0.99925889 0.99925889 0.99925889\n",
      "  0.99925889 0.99925889 0.99925889 0.99925889]\n",
      " [0.87178854 0.93305336 0.95133399 0.96170949 0.96640316 0.97134387\n",
      "  0.97579051 0.97801383 0.97949605 0.98048419 0.98097826 0.98270751\n",
      "  0.98320158 0.98468379 0.98517787 0.98517787 0.9854249  0.98517787\n",
      "  0.98567194 0.98616601 0.98616601 0.98690711 0.98740119 0.98740119\n",
      "  0.98764822 0.98814229 0.98838933 0.98863636 0.98913043 0.98913043\n",
      "  0.98913043 0.98937747 0.98913043 0.98863636 0.98838933 0.98863636\n",
      "  0.98838933 0.98814229 0.98814229 0.98814229]]\n",
      "Best threshold for class 0: 0.39, f2: 0.9896245059288538\n",
      "----------------------------------------------\n",
      "Best threshold for class 1: 0.16, f2: 0.9997529644268774\n",
      "----------------------------------------------\n",
      "Best threshold for class 2: 0.27, f2: 0.9987648221343873\n",
      "----------------------------------------------\n",
      "Best threshold for class 3: 0.07, f2: 0.9995059288537549\n",
      "----------------------------------------------\n",
      "Best threshold for class 4: 0.02, f2: 0.9995059288537549\n",
      "----------------------------------------------\n",
      "Best threshold for class 5: 0.39, f2: 0.9908596837944664\n",
      "----------------------------------------------\n",
      "Best threshold for class 6: 0.31, f2: 0.9938241106719368\n",
      "----------------------------------------------\n",
      "Best threshold for class 7: 0.29, f2: 0.9997529644268774\n",
      "----------------------------------------------\n",
      "Best threshold for class 8: 0.31, f2: 0.9896245059288538\n",
      "----------------------------------------------\n",
      "Best threshold for class 9: 0.22, f2: 0.9948122529644269\n",
      "----------------------------------------------\n",
      "Best threshold for class 10: 0.38, f2: 0.9935770750988142\n",
      "----------------------------------------------\n",
      "Best threshold for class 11: 0.4, f2: 0.9960474308300395\n",
      "----------------------------------------------\n",
      "Best threshold for class 12: 0.38, f2: 0.9945652173913043\n",
      "----------------------------------------------\n",
      "Best threshold for class 13: 0.21, f2: 0.9940711462450593\n",
      "----------------------------------------------\n",
      "Best threshold for class 14: 0.08, f2: 1.0\n",
      "----------------------------------------------\n",
      "Best threshold for class 15: 0.03, f2: 1.0\n",
      "----------------------------------------------\n",
      "Best threshold for class 16: 0.39, f2: 0.9881422924901185\n",
      "----------------------------------------------\n",
      "[[0.86956522 0.92564229 0.94762846 0.95800395 0.96516798 0.96912055\n",
      "  0.97282609 0.97579051 0.97751976 0.97949605 0.98048419 0.98147233\n",
      "  0.98171937 0.98295455 0.98369565 0.98418972 0.98468379 0.9854249\n",
      "  0.9854249  0.98641304 0.98715415 0.98715415 0.98715415 0.98764822\n",
      "  0.98838933 0.98863636 0.98838933 0.98838933 0.98838933 0.98838933\n",
      "  0.98838933 0.98838933 0.98863636 0.98863636 0.98863636 0.98863636\n",
      "  0.98913043 0.98913043 0.98962451 0.98962451]\n",
      " [0.99827075 0.99901186 0.99925889 0.99925889 0.99925889 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99975296 0.99975296 0.99975296\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593]\n",
      " [0.98246047 0.99085968 0.99407115 0.99555336 0.99629447 0.99678854\n",
      "  0.99703557 0.99728261 0.99728261 0.99777668 0.99802372 0.99802372\n",
      "  0.99802372 0.99777668 0.99802372 0.99802372 0.99827075 0.99827075\n",
      "  0.99827075 0.99827075 0.99827075 0.99827075 0.99851779 0.99851779\n",
      "  0.99851779 0.99851779 0.99876482 0.99876482 0.99876482 0.99876482\n",
      "  0.99876482 0.99876482 0.99876482 0.99876482 0.99876482 0.99851779\n",
      "  0.99851779 0.99851779 0.99851779 0.99851779]\n",
      " [0.99283597 0.9965415  0.99777668 0.99802372 0.99851779 0.99925889\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593]\n",
      " [0.99925889 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593]\n",
      " [0.96368577 0.97134387 0.97455534 0.97850791 0.97974308 0.98097826\n",
      "  0.9819664  0.98270751 0.98295455 0.98369565 0.98394269 0.98418972\n",
      "  0.98517787 0.98591897 0.98567194 0.98666008 0.98740119 0.98764822\n",
      "  0.98838933 0.98838933 0.98838933 0.98838933 0.98913043 0.98913043\n",
      "  0.98913043 0.98913043 0.98913043 0.98987154 0.98987154 0.98987154\n",
      "  0.98987154 0.99036561 0.99036561 0.99036561 0.99036561 0.99036561\n",
      "  0.99061265 0.99036561 0.99085968 0.99085968]\n",
      " [0.97208498 0.97974308 0.98295455 0.98493083 0.98567194 0.98690711\n",
      "  0.98764822 0.98838933 0.98838933 0.9888834  0.98987154 0.99011858\n",
      "  0.99085968 0.99110672 0.99085968 0.99110672 0.99184783 0.9923419\n",
      "  0.9923419  0.9923419  0.99283597 0.99333004 0.993083   0.993083\n",
      "  0.993083   0.99333004 0.99357708 0.99357708 0.99357708 0.99357708\n",
      "  0.99382411 0.99357708 0.99382411 0.99333004 0.99357708 0.99333004\n",
      "  0.99333004 0.99382411 0.99357708 0.99382411]\n",
      " [0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99950593 0.99950593\n",
      "  0.99950593 0.99950593 0.99950593 0.99950593 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296]\n",
      " [0.87895257 0.9229249  0.9409585  0.95355731 0.96047431 0.9666502\n",
      "  0.97307312 0.97480237 0.97529644 0.97826087 0.97875494 0.98171937\n",
      "  0.98221344 0.98295455 0.98468379 0.98567194 0.98616601 0.98666008\n",
      "  0.98740119 0.98740119 0.98715415 0.98814229 0.98838933 0.98838933\n",
      "  0.9888834  0.98863636 0.98838933 0.98863636 0.98913043 0.9888834\n",
      "  0.98962451 0.98937747 0.98937747 0.98913043 0.98913043 0.98913043\n",
      "  0.98937747 0.98962451 0.98962451 0.98937747]\n",
      " [0.94268775 0.9666502  0.97603755 0.98073123 0.98295455 0.98591897\n",
      "  0.98789526 0.99036561 0.99160079 0.99184783 0.9923419  0.9923419\n",
      "  0.99283597 0.99382411 0.99407115 0.99431818 0.99456522 0.99456522\n",
      "  0.99456522 0.99456522 0.99456522 0.99481225 0.99481225 0.99456522\n",
      "  0.99456522 0.99481225 0.99481225 0.99456522 0.99456522 0.99456522\n",
      "  0.99456522 0.99456522 0.99456522 0.99456522 0.99456522 0.99431818\n",
      "  0.99431818 0.99431818 0.99456522 0.99456522]\n",
      " [0.93675889 0.95405138 0.96269763 0.96837945 0.97332016 0.97653162\n",
      "  0.97850791 0.97999012 0.98048419 0.98320158 0.98394269 0.98517787\n",
      "  0.9854249  0.98641304 0.98764822 0.98789526 0.98814229 0.98863636\n",
      "  0.9888834  0.98962451 0.99085968 0.99085968 0.99085968 0.99085968\n",
      "  0.99085968 0.99110672 0.99135375 0.99135375 0.99209486 0.99209486\n",
      "  0.99283597 0.993083   0.993083   0.99333004 0.99333004 0.99333004\n",
      "  0.99333004 0.99357708 0.99357708 0.99357708]\n",
      " [0.9666502  0.98147233 0.98690711 0.98987154 0.99085968 0.9923419\n",
      "  0.99357708 0.99382411 0.99407115 0.99456522 0.99481225 0.99481225\n",
      "  0.99481225 0.99481225 0.99505929 0.99505929 0.99481225 0.99505929\n",
      "  0.99505929 0.99505929 0.99481225 0.99505929 0.99530632 0.99530632\n",
      "  0.99505929 0.99505929 0.99505929 0.99505929 0.99481225 0.99481225\n",
      "  0.99481225 0.99481225 0.99481225 0.99481225 0.99505929 0.99530632\n",
      "  0.99555336 0.9958004  0.9958004  0.99604743]\n",
      " [0.95009881 0.9597332  0.96368577 0.96837945 0.97183794 0.97529644\n",
      "  0.97850791 0.97924901 0.9812253  0.98344862 0.98443676 0.98517787\n",
      "  0.98591897 0.98764822 0.98863636 0.9888834  0.98962451 0.99036561\n",
      "  0.99036561 0.99036561 0.99085968 0.99160079 0.99135375 0.99135375\n",
      "  0.99184783 0.99209486 0.9923419  0.9923419  0.99258893 0.9923419\n",
      "  0.99258893 0.99283597 0.99333004 0.99333004 0.99357708 0.99407115\n",
      "  0.99431818 0.99456522 0.99431818 0.99456522]\n",
      " [0.91600791 0.94935771 0.9666502  0.97356719 0.98048419 0.98443676\n",
      "  0.98616601 0.98789526 0.99011858 0.99110672 0.99209486 0.99209486\n",
      "  0.99209486 0.99258893 0.99283597 0.993083   0.99333004 0.99357708\n",
      "  0.99357708 0.99357708 0.99407115 0.99407115 0.99407115 0.99382411\n",
      "  0.99357708 0.99357708 0.99382411 0.99357708 0.99357708 0.99357708\n",
      "  0.99357708 0.99382411 0.99382411 0.99333004 0.99333004 0.99357708\n",
      "  0.99333004 0.99283597 0.99283597 0.99283597]\n",
      " [0.99925889 0.99925889 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 1.         1.         0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296 0.99975296 0.99975296\n",
      "  0.99975296 0.99975296 0.99975296 0.99975296]\n",
      " [0.99901186 0.99975296 1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.88068182 0.93008893 0.95083992 0.96195652 0.96986166 0.97381423\n",
      "  0.97677866 0.9777668  0.97924901 0.97974308 0.98073123 0.98171937\n",
      "  0.98246047 0.98295455 0.98320158 0.98369565 0.98418972 0.98468379\n",
      "  0.98517787 0.9854249  0.98567194 0.98616601 0.98715415 0.98690711\n",
      "  0.98666008 0.98690711 0.98715415 0.98715415 0.98715415 0.98715415\n",
      "  0.98715415 0.98690711 0.98715415 0.98740119 0.98764822 0.98789526\n",
      "  0.98789526 0.98789526 0.98814229 0.98789526]]\n"
     ]
    }
   ],
   "source": [
    "threshold_list = [0.5,0.45,0.4,0.35,0.3,0.25,0.2,0.15,0.1,0.05]\n",
    "threshold_list = [i/100 for i in range(1, 41)]\n",
    "num_classes = 17\n",
    "f2_list_1, best_threshold_per_class_1 = test_threshold_indiv_class(val1_dataloader, test, threshold_list, num_classes)\n",
    "f2_list_2, best_threshold_per_class_2 = test_threshold_indiv_class(val2_dataloader, test, threshold_list, num_classes)\n",
    "f2_list_3, best_threshold_per_class_3 = test_threshold_indiv_class(val3_dataloader, test, threshold_list, num_classes)\n",
    "f2_list_4, best_threshold_per_class_4 = test_threshold_indiv_class(val4_dataloader, test, threshold_list, num_classes)\n",
    "f2_list_5, best_threshold_per_class_5 = test_threshold_indiv_class(val5_dataloader, test, threshold_list, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39 0.06 0.11 0.14 0.02 0.39 0.31 0.01 0.3  0.39 0.35 0.24 0.37 0.33\n",
      " 0.33 0.05 0.37]\n",
      "[0.37 0.04 0.25 0.2  0.01 0.4  0.28 0.01 0.38 0.16 0.32 0.34 0.4  0.25\n",
      " 0.2  0.06 0.38]\n",
      "[0.37 0.05 0.33 0.09 0.01 0.39 0.27 0.03 0.38 0.15 0.39 0.34 0.4  0.26\n",
      " 0.05 0.02 0.26]\n",
      "[0.38 0.06 0.35 0.36 0.05 0.36 0.28 0.14 0.35 0.26 0.38 0.27 0.4  0.25\n",
      " 0.04 0.03 0.32]\n",
      "[0.39 0.16 0.27 0.07 0.02 0.39 0.31 0.29 0.31 0.22 0.38 0.4  0.38 0.21\n",
      " 0.08 0.03 0.39]\n",
      "[0.38 0.074 0.262 0.17200000000000001 0.030000000000000002 0.386\n",
      " 0.29000000000000004 0.15333333333333332 0.34400000000000003\n",
      " 0.23600000000000004 0.364 0.31800000000000006 0.38999999999999996 0.26\n",
      " 0.14 0.038 0.34400000000000003]\n"
     ]
    }
   ],
   "source": [
    "print(best_threshold_per_class_1)\n",
    "print(best_threshold_per_class_2)\n",
    "print(best_threshold_per_class_3)\n",
    "print(best_threshold_per_class_4)\n",
    "print(best_threshold_per_class_5)\n",
    "\n",
    "stacked = np.stack([best_threshold_per_class_1, best_threshold_per_class_2, best_threshold_per_class_3, best_threshold_per_class_4, best_threshold_per_class_5])\n",
    "masked = np.ma.masked_array(stacked, mask=(stacked==0.01))\n",
    "average = np.mean(masked, axis=0)\n",
    "print(average)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
