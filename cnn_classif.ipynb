{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import fbeta_score\n",
    "from itertools import cycle\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classif models: \n",
      " ['alexnet', 'convnext_base', 'convnext_large', 'convnext_small', 'convnext_tiny', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'efficientnet_v2_l', 'efficientnet_v2_m', 'efficientnet_v2_s', 'googlenet', 'inception_v3', 'maxvit_t', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'mobilenet_v2', 'mobilenet_v3_large', 'mobilenet_v3_small', 'regnet_x_16gf', 'regnet_x_1_6gf', 'regnet_x_32gf', 'regnet_x_3_2gf', 'regnet_x_400mf', 'regnet_x_800mf', 'regnet_x_8gf', 'regnet_y_128gf', 'regnet_y_16gf', 'regnet_y_1_6gf', 'regnet_y_32gf', 'regnet_y_3_2gf', 'regnet_y_400mf', 'regnet_y_800mf', 'regnet_y_8gf', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x8d', 'resnext101_64x4d', 'resnext50_32x4d', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'squeezenet1_0', 'squeezenet1_1', 'swin_b', 'swin_s', 'swin_t', 'swin_v2_b', 'swin_v2_s', 'swin_v2_t', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'vit_b_16', 'vit_b_32', 'vit_h_14', 'vit_l_16', 'vit_l_32', 'wide_resnet101_2', 'wide_resnet50_2']\n"
     ]
    }
   ],
   "source": [
    "all_models = torchvision.models.list_models()\n",
    "classification_models = torchvision.models.list_models(module=torchvision.models)\n",
    "# print(f\"all models: \\n {all_models}\")\n",
    "print(f\"classif models: \\n {classification_models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = {\n",
    "    # ResNet & DenseNet\n",
    "    'ResNet': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "\n",
    "    # EfficientNet_b1\n",
    "    'EfficientNet': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(240),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "\n",
    "    # RegNet\n",
    "    'RegNet': transforms.Compose([\n",
    "        transforms.Resize(232),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "\n",
    "    # ConvNext\n",
    "    'ConvNext': transforms.Compose([\n",
    "        transforms.Resize(236),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>haze primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>agriculture clear primary water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>agriculture clear habitation primary road</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40474</th>\n",
       "      <td>train_40474</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40475</th>\n",
       "      <td>train_40475</td>\n",
       "      <td>cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40476</th>\n",
       "      <td>train_40476</td>\n",
       "      <td>agriculture clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40477</th>\n",
       "      <td>train_40477</td>\n",
       "      <td>agriculture clear primary road</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40478</th>\n",
       "      <td>train_40478</td>\n",
       "      <td>agriculture cultivation partly_cloudy primary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40479 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        image_name                                           tags\n",
       "0          train_0                                   haze primary\n",
       "1          train_1                agriculture clear primary water\n",
       "2          train_2                                  clear primary\n",
       "3          train_3                                  clear primary\n",
       "4          train_4      agriculture clear habitation primary road\n",
       "...            ...                                            ...\n",
       "40474  train_40474                                  clear primary\n",
       "40475  train_40475                                         cloudy\n",
       "40476  train_40476                      agriculture clear primary\n",
       "40477  train_40477                 agriculture clear primary road\n",
       "40478  train_40478  agriculture cultivation partly_cloudy primary\n",
       "\n",
       "[40479 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/train_classes.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = set()\n",
    "for tags in df['tags'].str.split():\n",
    "    all_tags.update(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agriculture': 0, 'artisinal_mine': 1, 'bare_ground': 2, 'blooming': 3, 'blow_down': 4, 'clear': 5, 'cloudy': 6, 'conventional_mine': 7, 'cultivation': 8, 'habitation': 9, 'haze': 10, 'partly_cloudy': 11, 'primary': 12, 'road': 13, 'selective_logging': 14, 'slash_burn': 15, 'water': 16}\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "tag_to_idx = {tag: idx for idx, tag in enumerate(sorted(all_tags))}\n",
    "idx_to_tag = {idx: tag for tag, idx in tag_to_idx.items()}\n",
    "print(tag_to_idx)\n",
    "print(len(tag_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx, 0]\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_name}.jpg\")\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        tags = self.df.iloc[idx, 1].split()\n",
    "        labels = torch.zeros(len(tag_to_idx))\n",
    "        for tag in tags:\n",
    "            labels[tag_to_idx[tag]] = 1\n",
    "        \n",
    "        return image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def visualize_sample(dataset, idx):\n",
    "#     image, labels = dataset[idx]\n",
    "    \n",
    "#     # convert the image tensor to a PIL Image for display\n",
    "#     if isinstance(image, torch.Tensor):\n",
    "#         image = transforms.ToPILImage()(image)\n",
    "    \n",
    "#     # plot the image\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.imshow(image)\n",
    "#     plt.axis('off')\n",
    "    \n",
    "#     # get the labels\n",
    "#     present_labels = [idx_to_tag[i] for i, label in enumerate(labels) if label == 1]\n",
    "    \n",
    "#     # set the title with the labels\n",
    "#     plt.title(f\"Labels: {', '.join(present_labels)}\")\n",
    "#     plt.show()\n",
    "    \n",
    "#     print(f\"Image labels: {', '.join(present_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_dataset = MultiLabelImageDataset(csv_file=\"data/train_classes.csv\", img_dir=\"data/train-jpg\", transform=transforms['ResNet'])\n",
    "effnet_dataset = MultiLabelImageDataset(csv_file=\"data/train_classes.csv\", img_dir=\"data/train-jpg\", transform=transforms['EfficientNet'])\n",
    "regnet_dataset = MultiLabelImageDataset(csv_file=\"data/train_classes.csv\", img_dir=\"data/train-jpg\", transform=transforms['RegNet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_sample(dataset, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset):\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        list(range(len(dataset))), \n",
    "        test_size=0.1, \n",
    "        random_state=42\n",
    "    )\n",
    "    train_dataset = Subset(dataset, train_idx)\n",
    "    test_dataset = Subset(dataset, test_idx)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "resnet_train, resnet_test = split_dataset(resnet_dataset)\n",
    "effnet_train, effnet_test = split_dataset(effnet_dataset)\n",
    "regnet_train, regnet_test = split_dataset(regnet_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "resnet_train_loader = DataLoader(resnet_train, batch_size=batch_size, shuffle=True)\n",
    "resnet_test_loader = DataLoader(resnet_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "effnet_train_loader = DataLoader(effnet_train, batch_size=batch_size, shuffle=True)\n",
    "effnet_test_loader = DataLoader(effnet_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "regnet_train_loader = DataLoader(regnet_train, batch_size=batch_size, shuffle=True)\n",
    "regnet_test_loader = DataLoader(regnet_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torch import nn\n",
    "\n",
    "num_classes = 17\n",
    "\n",
    "def ResNetClassifier(num_classes):  \n",
    "    # load a pre-trained model\n",
    "    model_ft = models.resnet50(weights='DEFAULT')\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    \n",
    "    # freeze all the parameters in the network except the final layer\n",
    "    # for param in model_ft.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    \n",
    "    # replace the last fully connected layer\n",
    "    model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    return model_ft\n",
    "\n",
    "def DenseNetClassifier(num_classes):\n",
    "    # load a pre-trained model\n",
    "    model_ft = models.densenet121(weights='DEFAULT')\n",
    "    num_ftrs = model_ft.classifier.in_features\n",
    "    \n",
    "    # freeze all the parameters in the network except the final layer\n",
    "    # for param in model_ft.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    \n",
    "    # replace the last fully connected layer\n",
    "    model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "    return model_ft\n",
    "\n",
    "def EfficientNetClassifier(num_classes):\n",
    "    # load a pre-trained model\n",
    "    model_ft = models.efficientnet_b1(weights='DEFAULT')\n",
    "    # num_ftrs = model_ft.classifier.in_features\n",
    "    \n",
    "    # freeze all the parameters in the network except the final layer\n",
    "    # for param in model_ft.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    \n",
    "    # replace the last fully connected layer\n",
    "    model_ft.classifier = nn.Linear(1280, num_classes)\n",
    "    return model_ft\n",
    "\n",
    "def RegNetClassifier(num_classes):\n",
    "    # load a pre-trained model\n",
    "    model_ft = models.regnet_y_8gf(weights='DEFAULT')\n",
    "    print(model_ft)\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    \n",
    "    # freeze all the parameters in the network except the final layer\n",
    "    # for param in model_ft.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    \n",
    "    # replace the last fully connected layer\n",
    "    model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    return model_ft\n",
    "\n",
    "def ConvNextClassifier(num_classes):\n",
    "    # load a pre-trained model\n",
    "    model_ft = models.convnext_tiny(weights='DEFAULT')\n",
    "    num_ftrs = model_ft.classifier[2].in_features\n",
    "    \n",
    "    # freeze all the parameters in the network except the final layer\n",
    "    # for param in model_ft.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    \n",
    "    # replace the last fully connected layer\n",
    "    model_ft.classifier = nn.Sequential(\n",
    "        nn.Flatten(), \n",
    "        nn.Linear(num_ftrs, num_classes)\n",
    "        )\n",
    "    return model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, num_classes, ensemble_type='weighted'):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        \n",
    "        # initialize individual models\n",
    "        self.resnet = ResNetClassifier(num_classes)\n",
    "        self.effnet = EfficientNetClassifier(num_classes)\n",
    "        self.regnet = RegNetClassifier(num_classes)\n",
    "        \n",
    "        # ensemble type\n",
    "        self.ensemble_type = ensemble_type\n",
    "        \n",
    "        # weighted averaging\n",
    "        if ensemble_type == 'weighted':\n",
    "            self.weights = nn.Parameter(torch.ones(3) / 3)\n",
    "\n",
    "        # parameters for shepard's rule\n",
    "        self.a = 1.0\n",
    "        self.b = 1.0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # get predictions from each model\n",
    "        resnet_out = self.resnet(x)\n",
    "        effnet_out = self.effnet(x)\n",
    "        regnet_out = self.regnet(x)\n",
    "        \n",
    "        # ensemble strategies\n",
    "        if self.ensemble_type == 'voting':\n",
    "            # soft voting - average of predictions\n",
    "            return (resnet_out + effnet_out + regnet_out) / 3\n",
    "        \n",
    "        elif self.ensemble_type == 'weighted':\n",
    "            # weighted average of predictions\n",
    "            # normalize weights to sum to 1\n",
    "            normalized_weights = nn.functional.softmax(self.weights, dim=0)\n",
    "            \n",
    "            weighted_out = (\n",
    "                normalized_weights[0] * resnet_out + \n",
    "                normalized_weights[1] * effnet_out + \n",
    "                normalized_weights[2] * regnet_out\n",
    "            )\n",
    "            return weighted_out\n",
    "\n",
    "        elif self.ensemble_type == 'shepard':\n",
    "            distances = torch.stack([\n",
    "                -torch.max(torch.sigmoid(resnet_out), dim=1)[0],\n",
    "                -torch.max(torch.sigmoid(effnet_out), dim=1)[0],\n",
    "                -torch.max(torch.sigmoid(regnet_out), dim=1)[0]\n",
    "            ], dim=1)\n",
    "\n",
    "            shepard_weights = torch.exp(-self.a * torch.abs(distances) ** self.b)\n",
    "            shepard_weights = shepard_weights / shepard_weights.sum(dim=1, keepdim=True)\n",
    "\n",
    "            weighted_out = (\n",
    "                shepard_weights[:, 0].unsqueeze(1) * resnet_out +\n",
    "                shepard_weights[:, 1].unsqueeze(1) * effnet_out +\n",
    "                shepard_weights[:, 2].unsqueeze(1) * regnet_out\n",
    "            )\n",
    "            return weighted_out\n",
    "        \n",
    "        else:\n",
    "            return (resnet_out + effnet_out + regnet_out) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ResNetClassifier(num_classes)\n",
    "# model = DenseNetClassifier(num_classes)\n",
    "# model = EfficientNetClassifier(num_classes)\n",
    "# model = RegNetClassifier(num_classes)\n",
    "# model = ConvNextClassifier(num_classes)\n",
    "\n",
    "# model.to(device)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "#     size = len(dataloader.dataset)\n",
    "#     model.train()\n",
    "#     for batch, (X, y) in enumerate(dataloader):\n",
    "#         X, y = X.to(device), y.to(device)\n",
    "#         pred = model(X)\n",
    "#         loss = loss_fn(pred, y)\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         if batch % 64 == 0:\n",
    "#             loss, current = loss.item(), batch * batch_size + len(X)\n",
    "#             print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "# def test_loop(dataloader, model, loss_fn):\n",
    "#     model.eval()\n",
    "#     size = len(dataloader.dataset)\n",
    "#     num_batches = len(dataloader)\n",
    "#     test_loss, f2 = 0, 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for X, y in dataloader:\n",
    "#             X, y = X.to(device), y.to(device)\n",
    "#             pred = model(X)\n",
    "#             test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "#             # calculate f2 score\n",
    "#             pred_tags = torch.sigmoid(pred).cpu().numpy() > 0.24\n",
    "#             true_tags = y.cpu().numpy()\n",
    "#             f2 += fbeta_score(true_tags, pred_tags, beta=2, average='micro')\n",
    "\n",
    "#     test_loss /= num_batches\n",
    "#     f2 /= num_batches\n",
    "    \n",
    "#     print(f\"Test Error: \\n f2 score: {f2:.5f}, avg loss: {test_loss:>8f} \\n\")\n",
    "#     return f2, test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble_model(\n",
    "    resnet_train_loader, \n",
    "    effnet_train_loader,\n",
    "    regnet_train_loader,\n",
    "    resnet_test_loader, \n",
    "    effnet_test_loader,\n",
    "    regnet_test_loader,\n",
    "    num_classes, \n",
    "    epochs, \n",
    "    learning_rate, \n",
    "    threshold,\n",
    "    ensemble_type) :\n",
    "\n",
    "    model = EnsembleModel(num_classes, ensemble_type).to(device)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.75)\n",
    "    \n",
    "    # training loop\n",
    "    all_loss = []\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        \n",
    "        dataloaders = [resnet_train_loader, effnet_train_loader, regnet_train_loader]\n",
    "        max_loader_len = max(len(loader) for loader in dataloaders)\n",
    "        \n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_idx in range(max_loader_len):\n",
    "            # cycle through dataloaders\n",
    "            X_resnet, y_resnet = next(cycle(resnet_train_loader))\n",
    "            X_effnet, y_effnet = next(cycle(effnet_train_loader))\n",
    "            X_regnet, y_regnet = next(cycle(regnet_train_loader))\n",
    "            \n",
    "            X_resnet = X_resnet.to(device)\n",
    "            X_effnet = X_effnet.to(device)\n",
    "            X_regnet = X_regnet.to(device)\n",
    "            \n",
    "            y_resnet = y_resnet.to(device)\n",
    "            y_effnet = y_effnet.to(device)\n",
    "            y_regnet = y_regnet.to(device)\n",
    "            \n",
    "            # get predictions\n",
    "            resnet_out = model.resnet(X_resnet)\n",
    "            effnet_out = model.effnet(X_effnet)\n",
    "            regnet_out = model.regnet(X_regnet)\n",
    "            \n",
    "            # compute losses\n",
    "            loss_resnet = loss_fn(resnet_out, y_resnet)\n",
    "            loss_effnet = loss_fn(effnet_out, y_effnet)\n",
    "            loss_regnet = loss_fn(regnet_out, y_regnet)\n",
    "            \n",
    "            # total loss\n",
    "            loss = (loss_resnet + loss_effnet + loss_regnet) / 3\n",
    "            \n",
    "            # backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 64 == 0:\n",
    "                print(f\"loss: {loss.item():>7f}\")\n",
    "        \n",
    "        # validation loop\n",
    "        model.eval()\n",
    "        test_loss, f2 = 0, 0\n",
    "        num_batches = min(len(resnet_test_loader), len(effnet_test_loader), len(regnet_test_loader))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for (X_resnet, y_resnet), (X_effnet, y_effnet), (X_regnet, y_regnet) in zip(\n",
    "                resnet_test_loader, effnet_test_loader, regnet_test_loader\n",
    "            ):\n",
    "                X_resnet = X_resnet.to(device)\n",
    "                X_effnet = X_effnet.to(device)\n",
    "                X_regnet = X_regnet.to(device)\n",
    "                \n",
    "                y_resnet = y_resnet.to(device)\n",
    "                y_effnet = y_effnet.to(device)\n",
    "                y_regnet = y_regnet.to(device)\n",
    "                \n",
    "                # get model predictions\n",
    "                pred_resnet = model.resnet(X_resnet)\n",
    "                pred_effnet = model.effnet(X_effnet)\n",
    "                pred_regnet = model.regnet(X_regnet)\n",
    "                \n",
    "                # ensemble prediction\n",
    "                if model.ensemble_type == 'weighted':\n",
    "                    normalized_weights = F.softmax(model.weights, dim=0)\n",
    "                    pred = (\n",
    "                        normalized_weights[0] * pred_resnet + \n",
    "                        normalized_weights[1] * pred_effnet + \n",
    "                        normalized_weights[2] * pred_regnet\n",
    "                    )\n",
    "\n",
    "                elif model.ensemble_type == 'shepard':\n",
    "                    # shepard's rule dynamic weights\n",
    "                    distances = torch.stack([\n",
    "                        -torch.max(torch.sigmoid(pred_resnet), dim=1)[0],\n",
    "                        -torch.max(torch.sigmoid(pred_effnet), dim=1)[0],\n",
    "                        -torch.max(torch.sigmoid(pred_regnet), dim=1)[0]\n",
    "                    ], dim=1)  # Shape: [batch_size, num_models]\n",
    "                    \n",
    "                    # shepard weights\n",
    "                    shepard_weights = torch.exp(-model.a * torch.abs(distances) ** model.b)\n",
    "                    shepard_weights = shepard_weights / shepard_weights.sum(dim=1, keepdim=True)\n",
    "                    \n",
    "                    # ensemble prediction\n",
    "                    pred = (\n",
    "                        shepard_weights[:, 0].unsqueeze(1) * pred_resnet + \n",
    "                        shepard_weights[:, 1].unsqueeze(1) * pred_effnet + \n",
    "                        shepard_weights[:, 2].unsqueeze(1) * pred_regnet\n",
    "                    )\n",
    "\n",
    "                else:\n",
    "                    pred = (pred_resnet + pred_effnet + pred_regnet) / 3\n",
    "                \n",
    "                # compute test loss\n",
    "                test_loss += loss_fn(pred, y_resnet).item()\n",
    "                \n",
    "                # calculate f2 score\n",
    "                pred_tags = torch.sigmoid(pred).cpu().numpy() > threshold\n",
    "                true_tags = y_resnet.cpu().numpy()\n",
    "                f2 += fbeta_score(true_tags, pred_tags, beta=2, average='micro')\n",
    "        \n",
    "        test_loss /= num_batches\n",
    "        f2 /= num_batches\n",
    "        \n",
    "        print(f\"Test Error: \\n f2 score: {f2:.5f}, avg loss: {test_loss:>8f} \\n\")\n",
    "        all_loss.append(test_loss)\n",
    "\n",
    "        scheduler.step()\n",
    "        print(scheduler.get_last_lr())\n",
    "    \n",
    "    return model, all_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "epochs = 5\n",
    "threshold = 0.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegNet(\n",
      "  (stem): SimpleStemIN(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (trunk_output): Sequential(\n",
      "    (block1): AnyStage(\n",
      "      (block1-0): ResBottleneckBlock(\n",
      "        (proj): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 224, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (f): BottleneckTransform(\n",
      "          (a): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (b): Conv2dNormActivation(\n",
      "            (0): Conv2d(224, 224, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=4, bias=False)\n",
      "            (1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (se): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(224, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(8, 224, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (c): Conv2dNormActivation(\n",
      "            (0): Conv2d(224, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activation): ReLU(inplace=True)\n",
      "      )\n",
      "      (block1-1): ResBottleneckBlock(\n",
      "        (f): BottleneckTransform(\n",
      "          (a): Conv2dNormActivation(\n",
      "            (0): Conv2d(224, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (b): Conv2dNormActivation(\n",
      "            (0): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4, bias=False)\n",
      "            (1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (se): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(224, 56, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(56, 224, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (c): Conv2dNormActivation(\n",
      "            (0): Conv2d(224, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activation): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (block2): AnyStage(\n",
      "      (block2-0): ResBottleneckBlock(\n",
      "        (proj): Conv2dNormActivation(\n",
      "          (0): Conv2d(224, 448, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (f): BottleneckTransform(\n",
      "          (a): Conv2dNormActivation(\n",
      "            (0): Conv2d(224, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (b): Conv2dNormActivation(\n",
      "            (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=8, bias=False)\n",
      "            (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (se): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(448, 56, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(56, 448, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (c): Conv2dNormActivation(\n",
      "            (0): Conv2d(448, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activation): ReLU(inplace=True)\n",
      "      )\n",
      "      (block2-1): ResBottleneckBlock(\n",
      "        (f): BottleneckTransform(\n",
      "          (a): Conv2dNormActivation(\n",
      "            (0): Conv2d(448, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (b): Conv2dNormActivation(\n",
      "            (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
      "            (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (se): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(448, 112, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(112, 448, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (c): Conv2dNormActivation(\n",
      "            (0): Conv2d(448, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activation): ReLU(inplace=True)\n",
      "      )\n",
      "      (block2-2): ResBottleneckBlock(\n",
      "        (f): BottleneckTransform(\n",
      "          (a): Conv2dNormActivation(\n",
      "            (0): Conv2d(448, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (b): Conv2dNormActivation(\n",
      "            (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
      "            (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (se): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(448, 112, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(112, 448, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (c): Conv2dNormActivation(\n",
      "            (0): Conv2d(448, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activation): ReLU(inplace=True)\n",
      "      )\n",
      "      (block2-3): ResBottleneckBlock(\n",
      "        (f): BottleneckTransform(\n",
      "          (a): Conv2dNormActivation(\n",
      "            (0): Conv2d(448, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (b): Conv2dNormActivation(\n",
      "            (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
      "            (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (se): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(448, 112, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(112, 448, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (c): Conv2dNormActivation(\n",
      "            (0): Conv2d(448, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activation): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (block3): AnyStage(\n",
      "      (block3-0): ResBottleneckBlock(\n",
      "        (proj): Conv2dNormActivation(\n",
      "          (0): Conv2d(448, 896, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (f): BottleneckTransform(\n",
      "          (a): Conv2dNormActivation(\n",
      "            (0): Conv2d(448, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (b): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 896, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (se): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(896, 112, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(112, 896, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (c): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activation): ReLU(inplace=True)\n",
      "      )\n",
      "      (block3-1): ResBottleneckBlock(\n",
      "        (f): BottleneckTransform(\n",
      "          (a): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (b): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (se): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(896, 224, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(224, 896, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (c): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activation): ReLU(inplace=True)\n",
      "      )\n",
      "      (block3-2): ResBottleneckBlock(\n",
      "        (f): BottleneckTransform(\n",
      "          (a): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (b): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (se): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(896, 224, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(224, 896, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (c): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activation): ReLU(inplace=True)\n",
      "      )\n",
      "      (block3-3): ResBottleneckBlock(\n",
      "        (f): BottleneckTransform(\n",
      "          (a): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (b): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (se): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(896, 224, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(224, 896, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (c): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activation): ReLU(inplace=True)\n",
      "      )\n",
      "      (block3-4): ResBottleneckBlock(\n",
      "        (f): BottleneckTransform(\n",
      "          (a): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (b): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (se): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(896, 224, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(224, 896, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (c): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activation): ReLU(inplace=True)\n",
      "      )\n",
      "      (block3-5): ResBottleneckBlock(\n",
      "        (f): BottleneckTransform(\n",
      "          (a): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (b): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (se): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(896, 224, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(224, 896, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (c): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activation): ReLU(inplace=True)\n",
      "      )\n",
      "      (block3-6): ResBottleneckBlock(\n",
      "        (f): BottleneckTransform(\n",
      "          (a): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (b): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (se): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(896, 224, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(224, 896, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (c): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activation): ReLU(inplace=True)\n",
      "      )\n",
      "      (block3-7): ResBottleneckBlock(\n",
      "        (f): BottleneckTransform(\n",
      "          (a): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (b): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (se): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(896, 224, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(224, 896, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (c): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activation): ReLU(inplace=True)\n",
      "      )\n",
      "      (block3-8): ResBottleneckBlock(\n",
      "        (f): BottleneckTransform(\n",
      "          (a): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (b): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (se): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(896, 224, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(224, 896, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (c): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activation): ReLU(inplace=True)\n",
      "      )\n",
      "      (block3-9): ResBottleneckBlock(\n",
      "        (f): BottleneckTransform(\n",
      "          (a): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (b): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (se): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(896, 224, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(224, 896, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (c): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activation): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (block4): AnyStage(\n",
      "      (block4-0): ResBottleneckBlock(\n",
      "        (proj): Conv2dNormActivation(\n",
      "          (0): Conv2d(896, 2016, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2016, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (f): BottleneckTransform(\n",
      "          (a): Conv2dNormActivation(\n",
      "            (0): Conv2d(896, 2016, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(2016, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (b): Conv2dNormActivation(\n",
      "            (0): Conv2d(2016, 2016, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=36, bias=False)\n",
      "            (1): BatchNorm2d(2016, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(inplace=True)\n",
      "          )\n",
      "          (se): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(2016, 224, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(224, 2016, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU()\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (c): Conv2dNormActivation(\n",
      "            (0): Conv2d(2016, 2016, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(2016, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (activation): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2016, out_features=1000, bias=True)\n",
      ")\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.694110\n",
      "loss: 0.196723\n",
      "loss: 0.156773\n",
      "loss: 0.118444\n",
      "loss: 0.115872\n",
      "loss: 0.118943\n",
      "loss: 0.125735\n",
      "loss: 0.109251\n",
      "loss: 0.106779\n",
      "loss: 0.112316\n",
      "loss: 0.095431\n",
      "loss: 0.099698\n",
      "loss: 0.091985\n",
      "loss: 0.113562\n",
      "loss: 0.087601\n",
      "loss: 0.110276\n",
      "loss: 0.107389\n",
      "loss: 0.108498\n",
      "Test Error: \n",
      " f2 score: 0.91892, avg loss: 0.083826 \n",
      "\n",
      "[7.500000000000001e-05]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.092166\n",
      "loss: 0.088606\n",
      "loss: 0.086503\n",
      "loss: 0.069752\n",
      "loss: 0.090632\n",
      "loss: 0.095897\n",
      "loss: 0.066209\n",
      "loss: 0.071420\n",
      "loss: 0.074285\n",
      "loss: 0.085435\n",
      "loss: 0.096603\n",
      "loss: 0.067411\n",
      "loss: 0.068087\n",
      "loss: 0.074322\n",
      "loss: 0.076411\n",
      "loss: 0.073202\n",
      "loss: 0.087543\n",
      "loss: 0.080903\n",
      "Test Error: \n",
      " f2 score: 0.91958, avg loss: 0.084292 \n",
      "\n",
      "[5.6250000000000005e-05]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.064052\n",
      "loss: 0.075371\n",
      "loss: 0.055422\n",
      "loss: 0.074958\n",
      "loss: 0.078265\n",
      "loss: 0.072637\n",
      "loss: 0.063353\n",
      "loss: 0.050927\n",
      "loss: 0.072218\n",
      "loss: 0.047607\n",
      "loss: 0.068049\n",
      "loss: 0.050940\n",
      "loss: 0.065395\n",
      "loss: 0.058894\n",
      "loss: 0.081700\n",
      "loss: 0.057597\n",
      "loss: 0.053410\n",
      "loss: 0.074994\n",
      "Test Error: \n",
      " f2 score: 0.91827, avg loss: 0.084872 \n",
      "\n",
      "[4.21875e-05]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.067280\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ensemble_model, loss_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ensemble_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresnet_train_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43meffnet_train_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregnet_train_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresnet_test_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43meffnet_test_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregnet_test_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensemble_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweighted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 58\u001b[0m, in \u001b[0;36mtrain_ensemble_model\u001b[0;34m(resnet_train_loader, effnet_train_loader, regnet_train_loader, resnet_test_loader, effnet_test_loader, regnet_test_loader, num_classes, epochs, learning_rate, threshold, ensemble_type)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# backprop\u001b[39;00m\n\u001b[1;32m     57\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 58\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     61\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/code/cv-1d-project/.venv/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/cv-1d-project/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/cv-1d-project/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ensemble_model, loss_history = train_ensemble_model(\n",
    "    resnet_train_loader,\n",
    "    effnet_train_loader,\n",
    "    regnet_train_loader,\n",
    "    resnet_test_loader,\n",
    "    effnet_test_loader,\n",
    "    regnet_test_loader,\n",
    "    num_classes=num_classes,\n",
    "    epochs=epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    threshold=threshold,\n",
    "    ensemble_type='weighted'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# all_loss = []\n",
    "# for t in range(epochs):\n",
    "#     print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "#     train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "#     f2, test_loss = test_loop(test_dataloader, model, loss_fn)\n",
    "#     all_loss.append(test_loss)\n",
    "# print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_loss)\n",
    "\n",
    "epochs_list = list(range(1, len(all_loss) + 1))\n",
    "# print(len(all_loss))\n",
    "# print(epochs)\n",
    "\n",
    "plt.plot(epochs_list, all_loss, marker='o', color='b', label='Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(model, image_path, transform, idx_to_tag):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        probabilities = torch.sigmoid(outputs)\n",
    "        predicted = probabilities > 0.24\n",
    "        predicted_labels = [idx_to_tag[i] for i, pred in enumerate(predicted[0]) if pred]\n",
    "\n",
    "    return predicted_labels, probabilities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"data/test-jpg/test_5689.jpg\"\n",
    "predicted_labels, probabilities = predict_image(model, image_path, transform, idx_to_tag)\n",
    "\n",
    "print(\"Predicted labels:\", predicted_labels)\n",
    "print(\"Probabilities:\")\n",
    "for i, prob in enumerate(probabilities):\n",
    "    if prob > 0.24:\n",
    "        print(f\"{idx_to_tag[i]}: {prob.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
